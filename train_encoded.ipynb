{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file in Google Drive\n",
    "zip_file_path = r'C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\data\\Huron_data.zip'\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(r\"C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'safetensors-convert.hf.space'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight is frozen\n",
      "model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias is frozen\n",
      "model.pixel_level_module.encoder.embeddings.norm.weight is frozen\n",
      "model.pixel_level_module.encoder.embeddings.norm.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight is frozen\n",
      "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage1.weight is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage1.bias is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage2.weight is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage2.bias is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage3.weight is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage3.bias is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage4.weight is frozen\n",
      "model.pixel_level_module.encoder.hidden_states_norms.stage4.bias is frozen\n",
      "model.pixel_level_module.decoder.level_embed is trainable\n",
      "model.pixel_level_module.decoder.input_projections.0.0.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.0.0.bias is trainable\n",
      "model.pixel_level_module.decoder.input_projections.0.1.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.0.1.bias is trainable\n",
      "model.pixel_level_module.decoder.input_projections.1.0.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.1.0.bias is trainable\n",
      "model.pixel_level_module.decoder.input_projections.1.1.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.1.1.bias is trainable\n",
      "model.pixel_level_module.decoder.input_projections.2.0.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.2.0.bias is trainable\n",
      "model.pixel_level_module.decoder.input_projections.2.1.weight is trainable\n",
      "model.pixel_level_module.decoder.input_projections.2.1.bias is trainable\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.fc1.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.fc1.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.fc2.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.fc2.bias is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight is frozen\n",
      "model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias is frozen\n",
      "model.pixel_level_module.decoder.mask_projection.weight is trainable\n",
      "model.pixel_level_module.decoder.mask_projection.bias is trainable\n",
      "model.pixel_level_module.decoder.adapter_1.0.weight is trainable\n",
      "model.pixel_level_module.decoder.adapter_1.1.weight is trainable\n",
      "model.pixel_level_module.decoder.adapter_1.1.bias is trainable\n",
      "model.pixel_level_module.decoder.layer_1.0.weight is trainable\n",
      "model.pixel_level_module.decoder.layer_1.1.weight is trainable\n",
      "model.pixel_level_module.decoder.layer_1.1.bias is trainable\n",
      "model.transformer_module.queries_embedder.weight is trainable\n",
      "model.transformer_module.queries_features.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.0.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.0.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.1.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.1.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.2.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.2.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.3.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.3.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.4.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.4.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.5.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.5.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.6.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.6.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.7.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.7.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.k_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.k_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.v_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.v_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.q_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.q_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.fc1.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.fc1.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.fc2.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.fc2.bias is trainable\n",
      "model.transformer_module.decoder.layers.8.final_layer_norm.weight is trainable\n",
      "model.transformer_module.decoder.layers.8.final_layer_norm.bias is trainable\n",
      "model.transformer_module.decoder.layernorm.weight is trainable\n",
      "model.transformer_module.decoder.layernorm.bias is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight is trainable\n",
      "model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias is trainable\n",
      "model.transformer_module.level_embed.weight is trainable\n",
      "class_predictor.weight is trainable\n",
      "class_predictor.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_model_and_cache():\n",
    "    \"\"\"Utility function to delete existing model and optimizer objects and clear GPU memory.\"\"\"\n",
    "    if 'model' in globals():\n",
    "        print(\"Deleting existing model...\")\n",
    "        del globals()['optimizer']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clear any existing model and cache\n",
    "\n",
    "# clear_model_and_cache()\n",
    "\n",
    "# Load the image processor with relevant settings\n",
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\n",
    "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "    do_rescale=True,   # Skip rescaling if images are already normalized\n",
    "    do_normalize=False,  # DO NOT NORMALIZE. POOR RESULTS FOR BINARY SEGMENATATION.\n",
    "    do_resize=True     # Skip resizing as we're handling this during preprocessing\n",
    ")\n",
    "\n",
    "# Load the Mask2Former model for binary segmentation\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "    num_labels=2,                     # Binary segmentation (background and tissue)\n",
    "    ignore_mismatched_sizes=True      # Allow resizing of model parameters if dimensions do not match\n",
    ")\n",
    "\n",
    "###############################################\n",
    "# Freezing encoder backbone if desired\n",
    "###############################################\n",
    "\n",
    "\n",
    "# Freeze the backbone of the Mask2Former model\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder\" in name:  # Match all layers within the encoder (backbone)\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Display the trainable layers for confirmation\n",
    "# Print trainable layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")\n",
    "    else:\n",
    "        print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Load the Conch model\n",
    "def load_model(hf_token):\n",
    "    encoder_model, encoder_preprocess = create_model_from_pretrained('conch_ViT-B-16', \"hf_hub:MahmoodLab/conch\", hf_auth_token=hf_token)\n",
    "    return encoder_model, encoder_preprocess\n",
    "\n",
    "def image_to_embedding(encoder_model, encoder_preprocess, image):\n",
    "    # Assume image is a PIL.Image.Image object\n",
    "    image = encoder_preprocess(image).unsqueeze(0)\n",
    "    with torch.inference_mode():\n",
    "        image_embs = encoder_model.encode_image(image, proj_contrast=False, normalize=False)\n",
    "    return image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Define paths (update these paths based on where you extracted the files)\n",
    "image_folder = os.path.join(r\"C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\data\", \"Sliced_Images\")\n",
    "mask_folder = os.path.join(r\"C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\data\", \"Sliced_masks\")\n",
    "\n",
    "\n",
    "# Get sorted lists of image and mask files\n",
    "image_files = [os.path.join(image_folder, f) for f in sorted(os.listdir(image_folder))]\n",
    "mask_files = [os.path.join(mask_folder, f) for f in sorted(os.listdir(mask_folder))]\n",
    "\n",
    "# Ensure matching number of images and masks\n",
    "assert len(image_files) == len(mask_files), \"Mismatch between image and mask files.\"\n",
    "\n",
    "def crop_black_borders(image, threshold=30):\n",
    "    \"\"\"Crop black borders from an image based on a threshold for black pixels.\"\"\"\n",
    "    img_array = np.array(image)\n",
    "    gray_img = np.mean(img_array, axis=2)  # Convert to grayscale by averaging channels\n",
    "\n",
    "    # Initialize cropping boundaries\n",
    "    top, bottom = 0, gray_img.shape[0]\n",
    "    left, right = 0, gray_img.shape[1]\n",
    "\n",
    "    # Crop from the top\n",
    "    while top < bottom and np.mean(gray_img[top, :]) <= threshold:\n",
    "        top += 1\n",
    "\n",
    "    # Crop from the bottom\n",
    "    while bottom > top and np.mean(gray_img[bottom - 1, :]) <= threshold:\n",
    "        bottom -= 1\n",
    "\n",
    "    # Crop from the left\n",
    "    while left < right and np.mean(gray_img[:, left]) <= threshold:\n",
    "        left += 1\n",
    "\n",
    "    # Crop from the right\n",
    "    while right > left and np.mean(gray_img[:, right - 1]) <= threshold:\n",
    "        right -= 1\n",
    "\n",
    "    # Crop the image to the calculated bounds\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "def gaussian_blur(image, radius=2):\n",
    "    \"\"\"Apply Gaussian blur to an image to avoid capturing noise as tissue.\"\"\"\n",
    "    return image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    \"\"\"Preprocess an image: crop black borders, enhance contrast, and resize.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    cropped_image = crop_black_borders(image)\n",
    "\n",
    "    # blur to remove noise\n",
    "    blurred_image = gaussian_blur(cropped_image, 2) # increase radius if desire more blur(large neighborhood)\n",
    "\n",
    "    # Resize to the target size\n",
    "    resized_image = blurred_image.resize(target_size, Image.BICUBIC)\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def preprocess_mask(mask_path, target_size=(128, 128)):\n",
    "    \"\"\"Convert mask to binary, ensure tissue is white and background is black, and resize.\"\"\"\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "\n",
    "    # Apply binary threshold and ensure tissue is white, background is black\n",
    "    binary_mask = np.where(mask_array > 0, 1, 0).astype(np.uint8)  # Normalize mask to [0, 1]\n",
    "    binary_mask = Image.fromarray(binary_mask * 255)  # Convert back to PIL Image\n",
    "\n",
    "    # Resize to the target size using nearest-neighbor interpolation\n",
    "    resized_mask = binary_mask.resize(target_size, Image.NEAREST)\n",
    "\n",
    "    return resized_mask\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transforms for images and masks\n",
    "image_transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Dataset class\n",
    "class TissueDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, image_processor=None, mask_transform=None, encoder_model=None, encoder_preprocess=None):\n",
    "        \"\"\"\n",
    "        Initialize the TissueDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - image_files: List of image file paths.\n",
    "        - mask_files: List of mask file paths.\n",
    "        - image_processor: Preprocessing function for images (expects PIL input).\n",
    "        - mask_transform: Preprocessing function for masks.\n",
    "        \"\"\"\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.image_processor = image_processor\n",
    "        self.mask_transform = mask_transform\n",
    "        self.encoder_model = encoder_model\n",
    "        self.encoder_preprocess = encoder_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask as PIL images\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")  # Convert to PIL RGB\n",
    "        mask = Image.open(self.mask_files[idx]).convert(\"L\")  # Convert to PIL Grayscale\n",
    "\n",
    "        # process images\n",
    "        image = preprocess_image(self.image_files[idx])\n",
    "\n",
    "        # Process image using the image processor\n",
    "        if self.image_processor:\n",
    "            # Convert PIL image to tensor using the image processor\n",
    "            image = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Convert tensor back to PIL image\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = to_pil_image(image)\n",
    "\n",
    "        # Pass the processed image to the encoder\n",
    "        image = image_to_embedding(self.encoder_model, self.encoder_preprocess, image)\n",
    "\n",
    "        # Process the mask\n",
    "        mask = preprocess_mask(self.mask_files[idx])\n",
    "\n",
    "        # Process the mask using mask_transform (if provided)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\CONCH\\conch\\open_clip_custom\\factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "encoder_model, encoder_preprocess = load_model(HF_TOKEN)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TissueDataset(\n",
    "    image_files=image_files,\n",
    "    mask_files=mask_files,\n",
    "    image_processor=image_processor,  # Pass the image processor here\n",
    "    mask_transform=mask_transform,     # Pass the mask transform here\n",
    "    encoder_model = encoder_model,\n",
    "    encoder_preprocess = encoder_preprocess\n",
    ")\n",
    "\n",
    "# Cut dataset for testing (optional for quick parameter testing)\n",
    "dataset = torch.utils.data.Subset(dataset, range(0, len(dataset)//100))\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# Print length of train and val dataset to verify split (if dataset was cut)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.3866e-01, -4.3637e-01,  1.5219e-01, -9.4854e-01, -2.9490e-01,\n",
      "          1.3714e+00, -2.3541e+00,  5.4747e-01,  1.1407e-01, -4.7157e-01,\n",
      "          1.1510e+00, -3.2379e-02, -8.8899e-01,  3.3333e-01,  5.4398e-01,\n",
      "          1.3963e+00,  1.3039e+00,  1.1848e+00,  1.0799e+00, -1.5730e+00,\n",
      "         -2.6319e-01, -5.3458e-01, -1.1381e+00,  8.4084e-01,  3.5167e-01,\n",
      "          7.0477e-02, -6.2041e-01, -9.1227e-01,  1.2431e+00, -4.4717e-01,\n",
      "         -1.3750e+00, -1.2674e-02,  9.4027e-01,  8.8804e-01,  4.0689e-01,\n",
      "          4.0382e-01, -2.4387e+00, -7.0475e-01,  2.0571e-01,  5.0308e-01,\n",
      "         -1.1907e+00,  9.6487e-02,  3.9031e-02, -1.0627e+00,  2.9906e-01,\n",
      "         -8.3799e-02,  4.0786e-01,  8.3398e-01,  4.5577e-01, -1.4103e+00,\n",
      "          1.4503e+00, -6.4990e-01, -8.1930e-01, -9.4102e-01, -1.0718e+00,\n",
      "          2.2950e-03, -3.9294e-01,  5.6193e-01, -3.5617e-01, -8.5566e-01,\n",
      "         -9.4535e-01,  1.9826e+00,  5.0768e-01,  4.8236e-01,  1.6429e-01,\n",
      "         -6.1299e-01,  3.2793e-01, -7.2467e-01, -7.1026e-02,  2.3199e+00,\n",
      "          5.2990e-02, -9.4054e-01, -5.3821e-02,  1.5027e+00,  1.1694e+00,\n",
      "          1.0556e+00, -1.4730e+00, -2.5526e+00, -1.5340e+00,  1.3406e+00,\n",
      "         -1.6795e-01, -9.2361e-02, -3.1797e-01, -9.2511e-01,  7.1205e-01,\n",
      "         -5.4112e-01,  1.7218e+00, -7.3447e-01,  2.3670e-01,  1.4344e+00,\n",
      "          8.4229e-02, -7.0557e-01, -3.0858e-01, -2.0304e+00, -8.0841e-01,\n",
      "          1.9315e+00, -1.8306e+00,  1.1887e+00, -2.8412e-01, -1.8805e+00,\n",
      "         -5.5183e-01, -6.8901e-01,  1.3930e+00, -8.6837e-01, -1.0409e+00,\n",
      "          4.1011e-01,  7.5530e-02, -5.9087e-01,  1.7057e+00, -6.3836e-01,\n",
      "          1.0977e+00,  3.9235e-01,  5.8979e-01,  1.8615e+00,  1.0136e+00,\n",
      "         -4.2241e-01, -1.3158e+00,  6.6629e-01, -3.7448e-01, -1.2062e+00,\n",
      "         -1.0369e-01, -6.1130e-01, -2.6185e-01, -2.3130e-01,  8.6173e-01,\n",
      "          1.3248e+00,  1.0364e+00,  1.3409e-01,  3.5774e-01, -1.5342e-01,\n",
      "          2.2826e+00, -1.6979e+00,  1.4551e+00, -1.4558e-02,  5.9506e-01,\n",
      "         -2.3126e-01,  1.3899e+00, -3.7678e-01, -4.2511e-01,  4.4072e-01,\n",
      "          6.1622e-01, -4.1585e-02,  1.1435e+00, -1.0517e+00,  4.6480e-01,\n",
      "         -7.1521e-01, -1.0482e+00, -3.1944e-02, -4.8334e-01, -2.7737e+00,\n",
      "         -5.7519e-01,  1.7095e+00,  1.2666e+00,  5.2188e-01, -6.9255e-01,\n",
      "         -4.6412e-01,  1.3259e+00,  6.3824e-01,  1.8061e+00, -5.5855e-01,\n",
      "         -4.1934e-02,  5.5081e-01, -9.2986e-01, -1.0156e+00,  8.1808e-03,\n",
      "          1.2216e+00, -5.7602e-03,  7.3188e-01,  1.1254e+00,  1.2774e+00,\n",
      "          7.6360e-01,  1.8170e+00, -1.2825e+00, -1.4699e+00,  1.6115e+00,\n",
      "          1.2567e+00, -2.4902e-01,  4.1764e-01,  2.0220e-01,  4.3789e-01,\n",
      "          2.2916e-01, -1.6011e-01,  7.6533e-01,  7.2833e-01,  1.1544e+00,\n",
      "          9.8046e-01,  1.0974e+00, -2.7776e+00, -1.1520e-01, -4.1437e-01,\n",
      "          1.6199e+00,  4.2688e-01, -4.3193e-01, -3.7344e-01,  1.1896e-01,\n",
      "         -1.4671e+00, -4.8593e-01, -1.6900e+00, -4.6319e-01, -8.7579e-01,\n",
      "         -2.7130e+00,  9.4122e-01,  5.0637e-01,  8.9291e-01, -5.4057e-01,\n",
      "          6.3985e-01, -9.6948e-01,  7.8961e-01, -8.3893e-01, -1.5066e+00,\n",
      "          1.5346e-01,  1.5259e+00,  1.3365e+00,  2.0803e-01,  3.9016e-01,\n",
      "         -2.6683e-01,  1.4359e+00, -7.4019e-01, -1.0902e+00,  1.0996e+00,\n",
      "         -1.7667e-02,  1.2535e+00,  1.5702e-01,  1.2905e+00,  9.0470e-02,\n",
      "          1.2570e+00,  5.5955e-01,  7.0854e-02, -9.1755e-01,  1.2577e+00,\n",
      "          1.7192e-01,  1.7654e+00, -1.9938e-01,  4.1443e-01, -3.4389e-01,\n",
      "         -7.9659e-01,  1.3458e+00, -8.6716e-01,  6.9370e-01,  8.3694e-01,\n",
      "         -8.0323e-01, -5.6788e-01,  2.1804e-01,  5.2489e-01,  1.7523e+00,\n",
      "         -5.0538e-01,  9.2751e-01, -4.9099e-01,  1.6121e+00, -2.4058e-01,\n",
      "          4.0405e-01,  1.4444e+00,  1.4277e+00,  1.9990e-01,  6.4667e-01,\n",
      "          3.3172e-01,  5.1652e-01,  2.4134e-01, -1.5448e-01, -2.8630e-01,\n",
      "          5.7193e-01, -1.8505e-01, -6.6897e-01, -4.3494e-01, -6.0275e-01,\n",
      "          5.2464e-01, -9.7157e-01, -1.4732e-01, -4.0600e-01, -1.9902e-02,\n",
      "          6.2367e-01, -9.8720e-01,  1.2531e-01, -6.5424e-01, -4.1653e-01,\n",
      "          7.5087e-01, -1.0499e+00,  7.3924e-01, -9.6886e-01, -2.1170e-01,\n",
      "         -1.2690e+00, -1.0381e+00, -3.5475e-01, -1.6201e+00, -3.5246e-01,\n",
      "          8.1388e-01,  6.0093e-01,  4.7627e-01, -3.0785e-01,  5.1699e-01,\n",
      "          1.4350e-01, -8.9875e-02,  1.5489e+00, -8.8302e-01, -6.5149e-01,\n",
      "          1.1390e+00,  8.1463e-01, -1.1131e+00,  1.1081e+00,  1.9000e-01,\n",
      "         -2.5311e-01, -7.7938e-01,  4.0980e-01, -1.0583e+00,  2.3167e-01,\n",
      "          2.3468e-02, -1.7967e-01, -1.0073e+00,  2.2452e-01, -6.4018e-01,\n",
      "          2.4603e+00,  5.4671e-01, -1.8217e-02,  2.3452e-02,  5.8955e-01,\n",
      "         -3.4490e-01,  3.8291e-02,  6.7434e-01, -5.9903e-01, -8.6238e-01,\n",
      "          1.5148e+00,  2.4617e-01,  1.9873e-02,  9.9869e-01, -7.8431e-02,\n",
      "         -1.0837e+00, -1.0604e-01,  5.2632e-01,  2.7853e-01, -3.4168e-01,\n",
      "         -1.4638e+00,  3.7300e-01, -1.9179e+00,  1.1102e+00, -1.9939e+00,\n",
      "         -8.4469e-01,  5.1776e-02,  1.5565e+00,  1.8710e-01,  6.1536e-02,\n",
      "         -3.0789e+00,  4.6272e-01,  6.4691e-01,  7.2913e-01, -1.5878e+00,\n",
      "          1.4746e+00,  5.0455e-01, -1.1056e+00, -5.3421e-01, -5.2547e-01,\n",
      "         -1.8808e-02, -8.9772e-02,  1.2717e-01,  5.0455e-01,  1.0770e-01,\n",
      "         -1.7139e-01,  4.7669e-02,  1.3236e+00,  5.8819e-01, -8.4940e-01,\n",
      "          2.5638e-01, -6.5258e-01, -4.5402e-01, -9.9161e-01, -7.9467e-01,\n",
      "          3.1420e-01,  1.4054e+00,  9.0186e-01, -1.1012e-01, -1.2899e+00,\n",
      "          1.0586e-01,  1.5063e-01,  9.6949e-01,  2.1218e-01,  5.4246e-01,\n",
      "         -9.6294e-01,  7.4311e-02,  1.9664e-01, -5.2868e-01,  1.0317e+00,\n",
      "          7.4304e-01,  6.7971e-01,  4.2650e-01, -1.4396e+00, -4.1654e-01,\n",
      "          3.9332e-01,  1.0576e-01,  2.8499e-01, -5.7040e-01,  3.2066e-01,\n",
      "         -1.3469e+00,  6.1685e-01,  5.4496e-01,  6.2966e-01,  1.8828e-01,\n",
      "          1.2778e+00, -1.1536e+00, -1.4231e-01,  6.6886e-01,  2.4395e-01,\n",
      "          6.6526e-01, -1.1581e-02, -1.4348e+00, -2.1086e+00, -5.3961e-01,\n",
      "          1.1232e+00, -6.3998e-01,  1.6239e+00,  5.8932e-01,  4.9353e-01,\n",
      "         -1.0236e-01, -1.6401e+00, -6.2351e-01,  2.2492e+00,  9.6786e-01,\n",
      "          9.3680e-01, -8.8341e-01, -1.6441e-01, -2.1773e+00,  7.6950e-01,\n",
      "          8.1440e-01,  6.0010e-01, -1.6154e+00,  2.0032e-01,  1.9578e-01,\n",
      "         -1.0126e+00, -1.1312e+00, -9.9767e-01, -4.7209e-01,  1.1724e+00,\n",
      "         -3.3364e-01,  6.8834e-01, -8.9545e-01,  3.4632e-01,  6.4707e-01,\n",
      "         -3.3560e-01, -1.0120e-01,  1.0777e+00,  6.6934e-01, -1.8704e+00,\n",
      "         -1.8428e+00,  2.1214e+00,  6.3394e-01,  1.1887e+00, -2.5260e-01,\n",
      "         -1.6699e+00,  1.2140e-01, -1.0584e-01,  4.0188e-01, -1.6772e+00,\n",
      "         -1.7410e-01, -4.4618e-01,  1.5420e+00,  7.3972e-01,  2.5485e+00,\n",
      "          9.6438e-01, -4.1890e-01,  1.3631e-02, -3.3096e-01,  4.3708e-01,\n",
      "          3.8072e-01,  1.6668e+00, -1.6689e+00, -1.2617e+00, -1.7273e+00,\n",
      "         -1.6147e+00,  1.2580e+00, -2.2432e+00,  1.4323e+00, -3.5227e-01,\n",
      "         -3.1372e-01, -4.0965e-02,  7.0462e-02,  2.4316e+00, -6.7205e-01,\n",
      "         -6.0387e-01, -5.4176e-01, -4.7812e-01, -5.0161e-01, -4.1051e-01,\n",
      "         -9.0674e-01,  1.7456e-01,  7.8443e-01, -8.5454e-01, -3.4442e+00,\n",
      "         -7.1619e-01,  1.8296e+00, -6.6908e-01,  1.1232e+00,  3.3475e+00,\n",
      "          5.3877e-01, -3.1505e-01, -1.9446e+00,  3.3133e-01, -1.5607e+00,\n",
      "         -1.8437e+00, -1.1128e+00, -8.5991e-01, -1.6839e+00, -2.1106e-01,\n",
      "         -9.7188e-01, -1.0195e+00,  1.2081e+00,  8.8500e-01, -1.0450e+00,\n",
      "         -6.6169e-01,  8.1368e-01,  3.2667e-01, -1.8673e+00, -1.2379e+00,\n",
      "         -2.2847e-01, -7.6109e-01]])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "image, mask = train_dataset[0]\n",
    "\n",
    "print(image)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\data\\Sliced_Images\\1.png\n"
     ]
    }
   ],
   "source": [
    "print(image_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = TissueDataset(\n",
    "    image_files=[image_files[1]],\n",
    "    mask_files=[mask_files[1]],\n",
    "    image_processor=image_processor,  # Pass the image processor here\n",
    "    mask_transform=mask_transform,     # Pass the mask transform here\n",
    "    encoder_model = encoder_model,\n",
    "    encoder_preprocess = encoder_preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "image, mask = dataset2[0]\n",
    "\n",
    "print(image.shape)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Apply sigmoid to inputs if not already done\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        # Flatten\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_infer(predicted_mask, ground_truth_mask):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between the predicted mask and the ground truth mask.\n",
    "\n",
    "    Args:\n",
    "        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n",
    "        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        float: IoU score.\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n",
    "    union = np.logical_or(predicted_mask, ground_truth_mask).sum()\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_dice_infer(predicted_mask, ground_truth_mask):\n",
    "    \"\"\"\n",
    "    Calculate Dice Coefficient between the predicted mask and the ground truth mask.\n",
    "\n",
    "    Args:\n",
    "        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n",
    "        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        float: Dice Coefficient score.\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n",
    "    return (2 * intersection) / (predicted_mask.sum() + ground_truth_mask.sum()) if (predicted_mask.sum() + ground_truth_mask.sum()) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# combined dice and BCE loss function\n",
    "# Define Combined Dice and BCE Loss\n",
    "class CombinedDiceBCELoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth=1e-6):\n",
    "        super(CombinedDiceBCELoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Dice Loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        intersection = (probs * targets).sum(dim=(1, 2))  # Sum over spatial dimensions only\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / (probs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) + self.smooth)\n",
    "        dice_loss = dice_loss.mean()  # Average over the batch\n",
    "\n",
    "        # BCE Loss\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "\n",
    "        # Combine losses\n",
    "        return self.dice_weight * dice_loss + self.bce_weight * bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print the device\n",
    "print(device)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Set a smaller learning rate for fine-tuning\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Assuming DiceLoss is already defined in your environment\n",
    "criterion = DiceLoss()\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Training function with loss calculation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for pixel_values, masks in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\"):\n",
    "            # Move inputs and masks to the correct device\n",
    "            pixel_values = pixel_values.to(device, dtype=next(model.parameters()).dtype)  # Match model dtype\n",
    "            masks = masks.to(device, dtype=torch.float32)  # Ensure masks are on the correct device and float32\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n",
    "\n",
    "            # Resize logits to match masks\n",
    "            tissue_logits_resized = F.interpolate(\n",
    "                tissue_logits.unsqueeze(1),  # Add channel dimension\n",
    "                size=masks.shape[-2:],  # Match mask size\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(1)  # Remove channel dimension\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(tissue_logits_resized, masks)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validation function aligned with inference logic, including IoU and Dice metric calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained segmentation model.\n",
    "        val_loader: DataLoader providing validation images and ground truth masks.\n",
    "        criterion: Loss function for evaluation.\n",
    "        device: Computation device (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        avg_val_loss: Average validation loss.\n",
    "        avg_iou: Average IoU across the validation set.\n",
    "        avg_dice: Average Dice score across the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_dice = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ground_truth_masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "            # Move ground truth masks to device and convert to float\n",
    "            ground_truth_masks = ground_truth_masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=images.to(device))\n",
    "            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n",
    "\n",
    "            # Resize logits to match ground truth mask size\n",
    "            tissue_logits_resized = torch.sigmoid(F.interpolate(\n",
    "                tissue_logits.unsqueeze(1),  # Add channel dimension\n",
    "                size=ground_truth_masks.shape[-2:],  # Match mask size\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(1))  # Remove channel dimension\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(tissue_logits_resized, ground_truth_masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Convert predicted logits to binary masks\n",
    "            predicted_masks = (tissue_logits_resized > 0.5).cpu().numpy().astype(np.uint8)\n",
    "            ground_truth_masks_np = ground_truth_masks.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            # Compute metrics\n",
    "            for pred, gt in zip(predicted_masks, ground_truth_masks_np):\n",
    "                total_iou += calculate_iou_infer(pred, gt)\n",
    "                total_dice += calculate_dice_infer(pred, gt)\n",
    "                num_samples += 1\n",
    "\n",
    "    # Calculate average loss and metrics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_iou = total_iou / num_samples if num_samples > 0 else 0\n",
    "    avg_dice = total_dice / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Mean IoU: {avg_iou:.4f}\")\n",
    "    print(f\"Mean Dice: {avg_dice:.4f}\")\n",
    "\n",
    "    return avg_val_loss, avg_iou, avg_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Training:   0%|          | 0/9 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     30\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pixel_values, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# Move inputs and masks to the correct device\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# Match model dtype\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Ensure masks are on the correct device and float32\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[50], line 43\u001b[0m, in \u001b[0;36mTissueDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     40\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_pil_image(image)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Pass the processed image to the encoder\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_preprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Process the mask\u001b[39;00m\n\u001b[0;32m     46\u001b[0m mask \u001b[38;5;241m=\u001b[39m preprocess_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_files[idx])\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mimage_to_embedding\u001b[1;34m(encoder_model, encoder_preprocess, image)\u001b[0m\n\u001b[0;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m encoder_preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 19\u001b[0m     image_embs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_contrast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_embs\n",
      "File \u001b[1;32m~\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\CONCH\\conch\\open_clip_custom\\coca_model.py:222\u001b[0m, in \u001b[0;36mCoCa.encode_image\u001b[1;34m(self, images, normalize, proj_contrast)\u001b[0m\n\u001b[0;32m    220\u001b[0m     image_latent, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_image(images, normalize\u001b[38;5;241m=\u001b[39mnormalize)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     image_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_no_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_latent\n",
      "File \u001b[1;32m~\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\CONCH\\conch\\open_clip_custom\\vision_tower.py:122\u001b[0m, in \u001b[0;36mVisualModel.forward_no_head\u001b[1;34m(self, x, normalize)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_no_head\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 122\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk_kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_attentional_pool_contrast:\n\u001b[0;32m    124\u001b[0m         pooled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_pool_contrast(x)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\timm\\models\\vision_transformer.py:810\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    808\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\timm\\models\\vision_transformer.py:166\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[1;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\timm\\layers\\mlp.py:46\u001b[0m, in \u001b[0;36mMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\CE126828\\OneDrive - Co-operators\\school\\Comp433\\Huron-Tissue-Segment\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, criterion, optimizer, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
