{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "image_folder = os.path.join(\"data\", \"Sliced_Images\")\n",
    "mask_folder = os.path.join(\"data\", \"Sliced_masks\")\n",
    "\n",
    "# Get sorted lists of image and mask files\n",
    "image_files = sorted(os.listdir(image_folder))\n",
    "mask_files = sorted(os.listdir(mask_folder))\n",
    "\n",
    "# Ensure matching number of images and masks\n",
    "assert len(image_files) == len(mask_files), \"Mismatch between image and mask files.\"\n",
    "\n",
    "def crop_black_borders(image, threshold=30):\n",
    "    \"\"\"Crop black borders from an image based on a threshold for black pixels.\"\"\"\n",
    "    img_array = np.array(image)\n",
    "    gray_img = np.mean(img_array, axis=2)  # Convert to grayscale by averaging channels\n",
    "\n",
    "    # Initialize cropping boundaries\n",
    "    top, bottom = 0, gray_img.shape[0]\n",
    "    left, right = 0, gray_img.shape[1]\n",
    "\n",
    "    # Crop from the top\n",
    "    while top < bottom and np.mean(gray_img[top, :]) <= threshold:\n",
    "        top += 1\n",
    "\n",
    "    # Crop from the bottom\n",
    "    while bottom > top and np.mean(gray_img[bottom - 1, :]) <= threshold:\n",
    "        bottom -= 1\n",
    "\n",
    "    # Crop from the left\n",
    "    while left < right and np.mean(gray_img[:, left]) <= threshold:\n",
    "        left += 1\n",
    "\n",
    "    # Crop from the right\n",
    "    while right > left and np.mean(gray_img[:, right - 1]) <= threshold:\n",
    "        right -= 1\n",
    "\n",
    "    # Crop the image to the calculated bounds\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess an image: crop black borders and enhance contrast.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    cropped_image = crop_black_borders(image)\n",
    "\n",
    "    # Enhance contrast\n",
    "    enhancer = ImageEnhance.Contrast(cropped_image)\n",
    "    enhanced_image = enhancer.enhance(10)\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "def preprocess_mask(mask_path):\n",
    "    \"\"\"Convert mask to binary and ensure tissue is white and background is black.\"\"\"\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
    "    mask_array = np.array(mask)\n",
    "\n",
    "    # Apply binary threshold and ensure tissue is white, background is black\n",
    "    binary_mask = np.where(mask_array > 0, 1, 0).astype(np.uint8)  # Normalize mask to [0, 1]\n",
    "    return Image.fromarray(binary_mask * 255)  # Return a PIL Image with values 0 or 255\n",
    "\n",
    "class TissueDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, image_folder, mask_folder, transform=None, mask_transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_folder, self.mask_files[idx])\n",
    "\n",
    "        image = preprocess_image(img_path)\n",
    "        mask = preprocess_mask(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Mask transform - avoid normalization if not needed\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = TissueDataset(\n",
    "    image_files, mask_files,\n",
    "    image_folder, mask_folder,\n",
    "    transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_batch_from_loader(loader, num_batches=1):\n",
    "    \"\"\"Visualize a few batches of images and masks from the DataLoader.\"\"\"\n",
    "    loader_iter = iter(loader)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get the next batch of images and masks\n",
    "        images, masks = next(loader_iter)\n",
    "        \n",
    "        # Move tensors to CPU if necessary and convert to numpy\n",
    "        images_np = images.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        masks_np = masks.squeeze(1).cpu().numpy()  # Remove channel dimension for masks\n",
    "\n",
    "        # Display images and masks side by side\n",
    "        batch_size = images_np.shape[0]\n",
    "        fig, axes = plt.subplots(batch_size, 2, figsize=(10, 5 * batch_size))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            axes[i, 0].imshow(images_np[i])\n",
    "            axes[i, 0].set_title(f\"Image {batch_idx * batch_size + i + 1}\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            \n",
    "            axes[i, 1].imshow(masks_np[i], cmap=\"gray\")\n",
    "            axes[i, 1].set_title(f\"Mask {batch_idx * batch_size + i + 1}\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Use this function to visualize a few batches of images and masks from the train_loader\n",
    "visualize_batch_from_loader(train_loader, num_batches=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def verify_data_alignment(dataset, num_samples=5):\n",
    "    \"\"\"Verify data processing by displaying original images, preprocessed images, and processed masks.\"\"\"\n",
    "    for i in range(num_samples):\n",
    "        # Retrieve the preprocessed image and mask\n",
    "        preprocessed_image, processed_mask = dataset[i]\n",
    "\n",
    "        # Convert tensors to PIL images for visualization\n",
    "        preprocessed_image_pil = transforms.ToPILImage()(preprocessed_image)\n",
    "        processed_mask_pil = transforms.ToPILImage()(processed_mask)\n",
    "\n",
    "        # Load the original image\n",
    "        original_image_path = os.path.join(image_folder, image_files[i])\n",
    "        original_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "\n",
    "        # Display the original image, preprocessed image, and processed mask side by side\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(original_image)\n",
    "        axs[0].set_title(f\"Original Image {i + 1}\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(preprocessed_image_pil)\n",
    "        axs[1].set_title(f\"Preprocessed Image {i + 1}\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(processed_mask_pil, cmap=\"gray\")\n",
    "        axs[2].set_title(f\"Processed Mask {i + 1}\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Run the verification for the first few images and masks\n",
    "verify_data_alignment(dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_model_and_cache():\n",
    "    \"\"\"Utility function to delete existing model and optimizer objects and clear GPU memory.\"\"\"\n",
    "    if 'model' in globals():\n",
    "        print(\"Deleting existing model...\")\n",
    "        del globals()['optimizer']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clear any existing model and cache\n",
    "clear_model_and_cache()\n",
    "\n",
    "# Load the image processor with relevant settings\n",
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\n",
    "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "    do_rescale=False,   # Skip rescaling if images are already normalized\n",
    "    do_normalize=True,  # Normalize images if needed\n",
    "    do_resize=False     # Skip resizing as we're handling this during preprocessing\n",
    ")\n",
    "\n",
    "# Load the Mask2Former model for binary segmentation\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "    num_labels=2,                     # Binary segmentation (background and tissue)\n",
    "    ignore_mismatched_sizes=True      # Allow resizing of model parameters if dimensions do not match\n",
    ")\n",
    "\n",
    "# Freeze all layers in the model initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the class predictor layer only\n",
    "for name, param in model.named_parameters():\n",
    "    if 'class_predictor' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Display the trainable layers for confirmation\n",
    "print(\"Trainable layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_modules\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Apply sigmoid to inputs if not already done\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Flatten\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# combined dice and BCE loss function\n",
    "# Define Combined Dice and BCE Loss\n",
    "class CombinedDiceBCELoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth=1e-6):\n",
    "        super(CombinedDiceBCELoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Dice Loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        intersection = (probs * targets).sum(dim=(1, 2))  # Sum over spatial dimensions only\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / (probs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) + self.smooth)\n",
    "        dice_loss = dice_loss.mean()  # Average over the batch\n",
    "\n",
    "        # BCE Loss\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "\n",
    "        # Combine losses\n",
    "        return self.dice_weight * dice_loss + self.bce_weight * bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_logits = torch.autograd.Variable(tissue_logits, requires_grad=True) #TEMPORARY FIX! THIS NEEDS TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print the device\n",
    "print(device)\n",
    "\n",
    "# Set a smaller learning rate for fine-tuning\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Assuming DiceLoss is already defined in your environment\n",
    "criterion = DiceLoss()\n",
    "\n",
    "# Helper functions\n",
    "def resize_mask(mask, target_shape=(512, 512)):\n",
    "    \"\"\"Resize a mask to the target shape using nearest neighbor interpolation.\"\"\"\n",
    "    mask = np.squeeze(mask).astype(np.uint8)\n",
    "    mask = Image.fromarray(mask).convert(\"L\")\n",
    "    resized_mask = mask.resize(target_shape, Image.NEAREST)\n",
    "    return np.array(resized_mask)\n",
    "\n",
    "def invert_mask(mask):\n",
    "    \"\"\"Invert binary mask (0 becomes 1 and vice versa).\"\"\"\n",
    "    return np.where(mask == 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "# Metric functions\n",
    "def iou_score(pred, target):\n",
    "    pred = (pred > 0).astype(np.uint8)\n",
    "    target = (target > 0).astype(np.uint8)\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "    return np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 1.0\n",
    "\n",
    "def dice_score(pred, target):\n",
    "    pred = (pred > 0).astype(np.uint8)\n",
    "    target = (target > 0).astype(np.uint8)\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection) / (np.sum(pred) + np.sum(target)) if (np.sum(pred) + np.sum(target) > 0) else 1.0\n",
    "\n",
    "# Unfreezing the class predictor and other necessary layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'class_predictor' in name:\n",
    "        param.requires_grad = True\n",
    "# send model to device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_iou, total_dice = 0, 0\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Process inputs and get model outputs\n",
    "            inputs = image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            tissue_logits = outputs.masks_queries_logits[:, 1].requires_grad_()  # Ensure tissue logits require gradients\n",
    "\n",
    "          \n",
    "            target_shape = tissue_logits.shape[-2:]  # Get the height, width from logits shape\n",
    "            \n",
    "            # Resize masks to match output shape\n",
    "            masks_resized = torch.stack([\n",
    "                torch.tensor(invert_mask(resize_mask(mask.cpu().numpy(), target_shape)), dtype=torch.float32, device=device)\n",
    "                for mask in masks\n",
    "            ])\n",
    "      \n",
    "\n",
    "            # Compute Dice loss\n",
    "            loss = criterion(tissue_logits, masks_resized)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU and Dice metrics\n",
    "            pred_mask = (torch.sigmoid(tissue_logits) > 0.5).float()\n",
    "            intersection = (pred_mask * masks_resized).sum((1, 2))\n",
    "            union = pred_mask.sum((1, 2)) + masks_resized.sum((1, 2)) - intersection\n",
    "            iou = (intersection / (union + 1e-6)).mean().item()\n",
    "            dice = (2 * intersection / (pred_mask.sum((1, 2)) + masks_resized.sum((1, 2)) + 1e-6)).mean().item()\n",
    "\n",
    "            total_iou += iou\n",
    "            total_dice += dice\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        avg_iou = total_iou / len(train_loader)\n",
    "        avg_dice = total_dice / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
    "\n",
    "# Validation function with shape debugging\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_iou, total_dice = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Process inputs and get model outputs\n",
    "            inputs = image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            tissue_logits = outputs.masks_queries_logits[:, 1]\n",
    "            \n",
    "           \n",
    "            target_shape = tissue_logits.shape[-2:]  # Get the height, width from logits shape\n",
    "            \n",
    "            # Resize masks to match output shape\n",
    "            masks_resized = torch.stack([\n",
    "                torch.tensor(invert_mask(resize_mask(mask.cpu().numpy(), target_shape)), dtype=torch.float32, device=device)\n",
    "                for mask in masks\n",
    "            ])\n",
    "    \n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(tissue_logits, masks_resized)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU and Dice metrics\n",
    "            pred_mask = (torch.sigmoid(tissue_logits) > 0.5).float()\n",
    "            intersection = (pred_mask * masks_resized).sum((1, 2))\n",
    "            union = pred_mask.sum((1, 2)) + masks_resized.sum((1, 2)) - intersection\n",
    "            iou = (intersection / (union + 1e-6)).mean().item()\n",
    "            dice = (2 * intersection / (pred_mask.sum((1, 2)) + masks_resized.sum((1, 2)) + 1e-6)).mean().item()\n",
    "\n",
    "            total_iou += iou\n",
    "            total_dice += dice\n",
    "        \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_iou = total_iou / len(val_loader)\n",
    "    avg_dice = total_dice / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
    "\n",
    "# Run training and validation\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "validate(model, val_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('fine-tuned-mask2former')\n",
    "image_processor.save_pretrained('fine-tuned-mask2former')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load thefine tuned model\n",
    "model_test = Mask2FormerForUniversalSegmentation.from_pretrained('fine-tuned-mask2former')\n",
    "image_processor_test = Mask2FormerImageProcessor.from_pretrained('fine-tuned-mask2former')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Resize and normalize mask\n",
    "def resize_and_normalize_mask(mask, target_shape=(512, 512)):\n",
    "    \"\"\"Resize a mask to the target shape and normalize values to [0, 1].\"\"\"\n",
    "    mask = np.squeeze(mask).astype(np.uint8)  # Remove extra dimensions and convert to uint8\n",
    "    mask_image = Image.fromarray(mask)\n",
    "    resized_mask = mask_image.resize(target_shape, Image.NEAREST)\n",
    "    return np.array(resized_mask) / 255  # Normalize to range [0, 1]\n",
    "\n",
    "# Invert mask (0 becomes 1, and 1 becomes 0)\n",
    "def invert_mask(mask):\n",
    "    mask = np.squeeze(mask).astype(np.uint8)  # Ensure mask is 2D and uint8\n",
    "    return np.where(mask == 0, 1, 0).astype(np.uint8)\n",
    "\n",
    "# Calculate IoU\n",
    "def calculate_iou(pred, target):\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "    return np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 1.0\n",
    "\n",
    "# Calculate Dice score\n",
    "def calculate_dice(pred, target):\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection) / (np.sum(pred) + np.sum(target)) if (np.sum(pred) + np.sum(target)) > 0 else 1.0\n",
    "\n",
    "# Run inference in batches\n",
    "def run_inference(model, image_processor, dataloader, device):\n",
    "    model.eval()\n",
    "    predicted_masks = []\n",
    "    ground_truth_masks = []\n",
    "    images_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Inference\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            inputs = image_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            mask_logits = outputs.masks_queries_logits\n",
    "            tissue_logits = mask_logits[:, 1]  # Select tissue logits\n",
    "\n",
    "            predicted_binary_masks = (torch.sigmoid(tissue_logits) > 0.5).cpu().numpy().astype(np.uint8)\n",
    "            predicted_masks.extend(predicted_binary_masks)\n",
    "            ground_truth_masks.extend(masks.cpu().numpy())\n",
    "            images_list.extend([img.cpu().numpy().transpose(1, 2, 0) for img in images])\n",
    "\n",
    "    return predicted_masks, ground_truth_masks, images_list\n",
    "\n",
    "# Evaluate segmentation with IoU and Dice calculations\n",
    "def evaluate_segmentation(predicted_masks, ground_truth_masks, target_shape=(512, 512)):\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "\n",
    "    for pred, target in zip(predicted_masks, ground_truth_masks):\n",
    "        pred_inverted = invert_mask(pred)  # Invert and ensure correct dtype and shape\n",
    "        target_inverted = invert_mask(target)\n",
    "\n",
    "        pred_resized = np.array(Image.fromarray(pred_inverted).resize(target_shape, Image.NEAREST))\n",
    "        target_resized = np.array(Image.fromarray(target_inverted).resize(target_shape, Image.NEAREST))\n",
    "\n",
    "        iou_score = calculate_iou(pred_resized, target_resized)\n",
    "        dice_score = calculate_dice(pred_resized, target_resized)\n",
    "\n",
    "        iou_scores.append(iou_score)\n",
    "        dice_scores.append(dice_score)\n",
    "\n",
    "    avg_iou = np.mean(iou_scores)\n",
    "    avg_dice = np.mean(dice_scores)\n",
    "\n",
    "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
    "    print(f\"Average Dice Coefficient: {avg_dice:.4f}\")\n",
    "    return dice_scores, iou_scores\n",
    "\n",
    "# Visualization to verify alignment\n",
    "def verify_alignment(images, predicted_masks, ground_truth_masks, num_samples=5, target_shape=(512, 512)):\n",
    "    for i in range(min(num_samples, len(predicted_masks))):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Original image\n",
    "        axs[0].imshow(images[i])\n",
    "        axs[0].set_title(f\"Image {i + 1}\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        # Predicted mask\n",
    "        pred_inverted = invert_mask(predicted_masks[i])\n",
    "        pred_resized = np.array(Image.fromarray(pred_inverted).resize(target_shape, Image.NEAREST))\n",
    "        axs[1].imshow(pred_resized, cmap='gray')\n",
    "        axs[1].set_title(f\"Predicted Mask {i + 1}\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        # Ground truth mask\n",
    "        gt_inverted = invert_mask(ground_truth_masks[i])\n",
    "        gt_resized = np.array(Image.fromarray(gt_inverted).resize(target_shape, Image.NEAREST))\n",
    "        axs[2].imshow(gt_resized, cmap='gray')\n",
    "        axs[2].set_title(f\"Ground Truth Mask {i + 1}\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Run inference\n",
    "predicted_masks, ground_truth_masks, images_list = run_inference(model, image_processor, train_loader, device)\n",
    "\n",
    "# Evaluate segmentation\n",
    "dice_scores, iou_scores = evaluate_segmentation(predicted_masks, ground_truth_masks)\n",
    "\n",
    "# Verify alignment visually\n",
    "verify_alignment(images_list, predicted_masks, ground_truth_masks, num_samples=50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth-pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
