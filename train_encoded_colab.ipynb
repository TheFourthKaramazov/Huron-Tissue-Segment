{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWGaxDSyAkzw",
        "outputId": "dbf615a1-f878-4ad1-d27c-76c5ef7fc9a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the ZIP file in Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/data/Huron_data.zip'\n",
        "\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Huron_data')"
      ],
      "metadata": {
        "id": "lgy6sWYIAsIt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab"
      ],
      "metadata": {
        "id": "n4ukMzEf2_W_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc3w5sjNAgcM",
        "outputId": "801ed494-f844-4e98-d7c0-57630996e890"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:07\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mahmoodlab/CONCH.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmmjC969ATOs",
        "outputId": "e475e3c2-2529-416c-9375-8df7359b1e48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CONCH'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 93 (delta 36), reused 30 (delta 28), pack-reused 35 (from 1)\u001b[K\n",
            "Receiving objects: 100% (93/93), 1.22 MiB | 3.87 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CONCH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5OMzWA_AYgo",
        "outputId": "fd5a5115-a9d7-44f9-9c94-bd74b4121caa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CONCH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n conch python=3.10 -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yxPefdrVNRxW",
        "outputId": "69abaf10-7c84-4118-a9ae-f1426ee7b543"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/conch\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.10\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bzip2-1.0.8                |       h4bc722e_7         247 KB  conda-forge\n",
            "    ca-certificates-2024.8.30  |       hbcca054_0         155 KB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h712a8e2_2         654 KB  conda-forge\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libsqlite-3.47.0           |       hadc24fc_1         855 KB  conda-forge\n",
            "    libxcrypt-4.4.36           |       hd590300_1          98 KB  conda-forge\n",
            "    libzlib-1.3.1              |       hb9d3cd8_2          60 KB  conda-forge\n",
            "    ncurses-6.5                |       he02047a_1         868 KB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    pip-24.3.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n",
            "    python-3.10.15             |h4a871b0_2_cpython        24.1 MB  conda-forge\n",
            "    setuptools-75.6.0          |     pyhff2d567_0         756 KB  conda-forge\n",
            "    tzdata-2024b               |       hc8b5060_0         119 KB  conda-forge\n",
            "    wheel-0.45.1               |     pyhd8ed1ab_0          62 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        33.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2024.8.30-hbcca054_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h712a8e2_2 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.2.0-h69a702a_1 \n",
            "  libgomp            conda-forge/linux-64::libgomp-14.2.0-h77fa898_1 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.47.0-hadc24fc_1 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-he02047a_1 \n",
            "  openssl            conda-forge/linux-64::openssl-3.4.0-hb9d3cd8_0 \n",
            "  pip                conda-forge/noarch::pip-24.3.1-pyh8b19718_0 \n",
            "  python             conda-forge/linux-64::python-3.10.15-h4a871b0_2_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n",
            "  setuptools         conda-forge/noarch::setuptools-75.6.0-pyhff2d567_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  tzdata             conda-forge/noarch::tzdata-2024b-hc8b5060_0 \n",
            "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_0 \n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.10.15       | 24.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "openssl-3.4.0        | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.47.0     | 855 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 155 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024b         | 119 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :   0% 0.0006470482511036923/1 [00:00<03:10, 190.18s/it]\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.47.0     | 855 KB    | :   2% 0.018717105977158824/1 [00:00<00:07,  8.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   1% 0.013179232412674715/1 [00:00<00:11, 11.76s/it]\u001b[A\u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | :   1% 0.005558673111072358/1 [00:00<00:32, 32.72s/it]\u001b[A\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :   9% 0.08993970690341324/1 [00:00<00:01,  2.08s/it]   \n",
            "openssl-3.4.0        | 2.8 MB    | :  30% 0.30016834799790737/1 [00:00<00:00,  1.31it/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | :   2% 0.019303795604097816/1 [00:00<00:14, 14.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.47.0     | 855 KB    | : 100% 1.0/1 [00:00<00:00,  3.97it/s]                 \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :  32% 0.32417117380294985/1 [00:00<00:00,  1.29it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | :   2% 0.021159647890234327/1 [00:00<00:14, 14.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | :   4% 0.0355407469110093/1 [00:00<00:10, 10.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :  46% 0.46005130653472526/1 [00:00<00:00,  1.26it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   6% 0.06481448515129577/1 [00:00<00:06,  6.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 155 KB    | :  10% 0.10304208096702577/1 [00:00<00:04,  4.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :  59% 0.5946373427642933/1 [00:00<00:00,  1.29it/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :  16% 0.163198629386511/1 [00:00<00:02,  3.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | :  26% 0.26007174830947016/1 [00:00<00:01,  2.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :  27% 0.2687531781572429/1 [00:00<00:01,  2.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | : 100% 1.0/1 [00:00<00:00,  1.96it/s]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | : 100% 1.0/1 [00:00<00:00,  1.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.15       | 24.1 MB   | :  92% 0.9162203235628283/1 [00:00<00:00,  1.48it/s]\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.03s/it]                 \u001b[A\u001b[A\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | : 100% 1.0/1 [00:01<00:00,  1.09s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | : 100% 1.0/1 [00:01<00:00,  1.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.20s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | : 100% 1.0/1 [00:01<00:00,  1.26s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | : 100% 1.0/1 [00:01<00:00,  1.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 155 KB    | : 100% 1.0/1 [00:01<00:00,  1.29s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 155 KB    | : 100% 1.0/1 [00:01<00:00,  1.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.55s/it]                \u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.55s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:01<00:00,  1.48s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:01<00:00,  1.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | : 100% 1.0/1 [00:01<00:00,  1.59s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.45.1         | 62 KB     | : 100% 1.0/1 [00:01<00:00,  1.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:01<00:00,  1.63s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:01<00:00,  1.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.2.0     | 53 KB     | : 100% 1.0/1 [00:01<00:00,  1.74s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.2.0     | 53 KB     | : 100% 1.0/1 [00:01<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024b         | 119 KB    | : 100% 1.0/1 [00:02<00:00,  1.97s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024b         | 119 KB    | : 100% 1.0/1 [00:02<00:00,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | : 100% 1.0/1 [00:02<00:00,  2.53s/it]                 \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate conch\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck43jvjSN7zK",
        "outputId": "a99f7bf5-1a80-4a97-e26a-3b9eba52ad11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no change     /usr/local/condabin/conda\n",
            "no change     /usr/local/bin/conda\n",
            "no change     /usr/local/bin/conda-env\n",
            "no change     /usr/local/bin/activate\n",
            "no change     /usr/local/bin/deactivate\n",
            "no change     /usr/local/etc/profile.d/conda.sh\n",
            "no change     /usr/local/etc/fish/conf.d/conda.fish\n",
            "no change     /usr/local/shell/condabin/Conda.psm1\n",
            "no change     /usr/local/shell/condabin/conda-hook.ps1\n",
            "no change     /usr/local/lib/python3.10/site-packages/xontrib/conda.xsh\n",
            "no change     /usr/local/etc/profile.d/conda.csh\n",
            "no change     /root/.bashrc\n",
            "No action taken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda activate conch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlUKC71NNr_-",
        "outputId": "fe9db158-4c80-47a0-dd30-762ab61b8015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CondaError: Run 'conda init' before 'conda activate'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QrgbdrTUPLWC",
        "outputId": "46671f09-1567-425d-f2d1-a091fa1c28dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/CONCH\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch>=2.0.1 (from conch==0.1.0)\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision (from conch==0.1.0)\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting transformers (from conch==0.1.0)\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers (from conch==0.1.0)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting numpy (from conch==0.1.0)\n",
            "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from conch==0.1.0)\n",
            "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting timm>=0.9.8 (from conch==0.1.0)\n",
            "  Downloading timm-1.0.11-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex (from conch==0.1.0)\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from conch==0.1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting h5py (from conch==0.1.0)\n",
            "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pandas (from conch==0.1.0)\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml (from timm>=0.9.8->conch==0.1.0)\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting huggingface_hub (from timm>=0.9.8->conch==0.1.0)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting safetensors (from timm>=0.9.8->conch==0.1.0)\n",
            "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting filelock (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting wcwidth (from ftfy->conch==0.1.0)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->conch==0.1.0)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->conch==0.1.0)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->conch==0.1.0)\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn->conch==0.1.0)\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->conch==0.1.0)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->conch==0.1.0)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->conch==0.1.0)\n",
            "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers->conch==0.1.0) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers->conch==0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers->conch==0.1.0) (4.66.1)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->conch==0.1.0)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.1->conch==0.1.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers->conch==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers->conch==0.1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers->conch==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers->conch==0.1.0) (2023.11.17)\n",
            "Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: conch\n",
            "  Building editable for conch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for conch: filename=conch-0.1.0-0.editable-py3-none-any.whl size=14141 sha256=d1baadaee25dd2cf3d1c18021892972e545bd335ecf8097e518e3ecaf057e0fe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j4gdl3w_/wheels/7c/06/81/dd5498774623954c83b07bd08c3bb3a3a7f564cdf3f118d2e8\n",
            "Successfully built conch\n",
            "Installing collected packages: wcwidth, pytz, mpmath, tzdata, typing-extensions, threadpoolctl, sympy, six, safetensors, regex, pyyaml, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, ftfy, fsspec, filelock, triton, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, h5py, tokenizers, scikit-learn, pandas, nvidia-cusolver-cu12, transformers, torch, torchvision, timm, conch\n",
            "Successfully installed MarkupSafe-3.0.2 conch-0.1.0 filelock-3.16.1 fsspec-2024.10.0 ftfy-6.3.1 h5py-3.12.1 huggingface_hub-0.26.2 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 pillow-11.0.0 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 six-1.16.0 sympy-1.13.1 threadpoolctl-3.5.0 timm-1.0.11 tokenizers-0.20.3 torch-2.5.1 torchvision-0.20.1 transformers-4.46.3 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 wcwidth-0.2.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "six",
                  "wcwidth"
                ]
              },
              "id": "7d312a26820c4bfdb6acf62d97b126ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda init\n",
        "\n",
        "%%bash\n",
        "source activate conch\n",
        "\n",
        "!conda activate conch\n",
        "\n",
        "!conda env list"
      ],
      "metadata": {
        "id": "tYK-n659Ynla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M46PyCIQzpOC"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CURL_CA_BUNDLE'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9a08567b369c41a48a430a710bf787c1",
            "8f2f8cbf2a774ea59db3b520da83637e",
            "3d434016e9b3462aba41852c643212e9",
            "5e2e5108da174d57943482a6d5a05801",
            "1af7289de1034fc78a60e9411ca9a54b",
            "f49716a162ed4ad2ae6db69f3bef0a4c",
            "68b9fbab27f444e28436868a57ef8cc6",
            "54539bf928b74db1b6d8bd109dd21887",
            "280747e87a6e4e768e6c5b1c595b6e27",
            "54c97cf33eed40168066de58083e91d3",
            "fbeea4c2168746c9aea4832b0777aaa4",
            "f91538a846f3428890481241acf482d3",
            "2fcd5741f1da4a3899b266889bdfac05",
            "1821cf23d0444e34aa95803babcb5873",
            "285db2e63ec54eb2be7c40d2360fd288",
            "384bbb1c26c94ed98f59bdbb2add761a",
            "445add1a257d43678cf9210ca5ae5085",
            "308038273f1e4f5598828b20e809d417",
            "4dd3bc2f675e481d855ec355f89e3019",
            "b60504b1f05e41768c63c36a99cfddb6",
            "fe63d4ca7eb24dcaa6c1b5f4e8df9655",
            "fa9ef4b0104b4b6ab4292aa24c24a831",
            "d468821243a349bf970a109108da9cb0",
            "e00aa9a539444e9591d2fbd24a9debd9",
            "8ef664a821f840199f344229197db753",
            "be2a5bed085d457f83f77885785122b1",
            "529382db43904f84a30d1ba4f46c410d",
            "693d31724d2e48f3930b8fb9c4862da9",
            "0ecc269b77a1462e9df0066c64fd80b2",
            "4e712b0cc27443368ed72fc021d8a31d",
            "94b77d537ec9480eaa339f0bffb745d9",
            "2079483623214bc7b8afbb2d467a56fd",
            "85a59e8b121b4f41a2900a27f91cc51a"
          ]
        },
        "id": "IVdY6dItzpOD",
        "outputId": "1197de76-2b5e-4b09-b424-e56ec2aa3295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a08567b369c41a48a430a710bf787c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/9.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f91538a846f3428890481241acf482d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/432M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d468821243a349bf970a109108da9cb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-ade-semantic and are newly initialized because the shapes did not match:\n",
            "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
            "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight is frozen\n",
            "model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias is frozen\n",
            "model.pixel_level_module.encoder.embeddings.norm.weight is frozen\n",
            "model.pixel_level_module.encoder.embeddings.norm.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight is frozen\n",
            "model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage1.weight is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage1.bias is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage2.weight is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage2.bias is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage3.weight is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage3.bias is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage4.weight is frozen\n",
            "model.pixel_level_module.encoder.hidden_states_norms.stage4.bias is frozen\n",
            "model.pixel_level_module.decoder.level_embed is trainable\n",
            "model.pixel_level_module.decoder.input_projections.0.0.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.0.0.bias is trainable\n",
            "model.pixel_level_module.decoder.input_projections.0.1.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.0.1.bias is trainable\n",
            "model.pixel_level_module.decoder.input_projections.1.0.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.1.0.bias is trainable\n",
            "model.pixel_level_module.decoder.input_projections.1.1.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.1.1.bias is trainable\n",
            "model.pixel_level_module.decoder.input_projections.2.0.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.2.0.bias is trainable\n",
            "model.pixel_level_module.decoder.input_projections.2.1.weight is trainable\n",
            "model.pixel_level_module.decoder.input_projections.2.1.bias is trainable\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.fc1.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.fc1.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.fc2.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.fc2.bias is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight is frozen\n",
            "model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias is frozen\n",
            "model.pixel_level_module.decoder.mask_projection.weight is trainable\n",
            "model.pixel_level_module.decoder.mask_projection.bias is trainable\n",
            "model.pixel_level_module.decoder.adapter_1.0.weight is trainable\n",
            "model.pixel_level_module.decoder.adapter_1.1.weight is trainable\n",
            "model.pixel_level_module.decoder.adapter_1.1.bias is trainable\n",
            "model.pixel_level_module.decoder.layer_1.0.weight is trainable\n",
            "model.pixel_level_module.decoder.layer_1.1.weight is trainable\n",
            "model.pixel_level_module.decoder.layer_1.1.bias is trainable\n",
            "model.transformer_module.queries_embedder.weight is trainable\n",
            "model.transformer_module.queries_features.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.0.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.0.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.1.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.1.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.2.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.2.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.3.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.3.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.4.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.4.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.5.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.5.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.6.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.6.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.7.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.7.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.k_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.k_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.v_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.v_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.q_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.q_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.fc1.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.fc1.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.fc2.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.fc2.bias is trainable\n",
            "model.transformer_module.decoder.layers.8.final_layer_norm.weight is trainable\n",
            "model.transformer_module.decoder.layers.8.final_layer_norm.bias is trainable\n",
            "model.transformer_module.decoder.layernorm.weight is trainable\n",
            "model.transformer_module.decoder.layernorm.bias is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight is trainable\n",
            "model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias is trainable\n",
            "model.transformer_module.level_embed.weight is trainable\n",
            "class_predictor.weight is trainable\n",
            "class_predictor.bias is trainable\n"
          ]
        }
      ],
      "source": [
        "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_model_and_cache():\n",
        "    \"\"\"Utility function to delete existing model and optimizer objects and clear GPU memory.\"\"\"\n",
        "    if 'model' in globals():\n",
        "        print(\"Deleting existing model...\")\n",
        "        del globals()['optimizer']\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Clear any existing model and cache\n",
        "\n",
        "# clear_model_and_cache()\n",
        "\n",
        "# Load the image processor with relevant settings\n",
        "image_processor = Mask2FormerImageProcessor.from_pretrained(\n",
        "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
        "    do_rescale=True,   # Skip rescaling if images are already normalized\n",
        "    do_normalize=False,  # DO NOT NORMALIZE. POOR RESULTS FOR BINARY SEGMENATATION.\n",
        "    do_resize=True     # Skip resizing as we're handling this during preprocessing\n",
        ")\n",
        "\n",
        "# Load the Mask2Former model for binary segmentation\n",
        "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
        "    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
        "    num_labels=2,                     # Binary segmentation (background and tissue)\n",
        "    ignore_mismatched_sizes=True      # Allow resizing of model parameters if dimensions do not match\n",
        ")\n",
        "\n",
        "###############################################\n",
        "# Freezing encoder backbone if desired\n",
        "###############################################\n",
        "\n",
        "\n",
        "# Freeze the backbone of the Mask2Former model\n",
        "for name, param in model.named_parameters():\n",
        "    if \"encoder\" in name:  # Match all layers within the encoder (backbone)\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Display the trainable layers for confirmation\n",
        "# Print trainable layers\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} is trainable\")\n",
        "    else:\n",
        "        print(f\"{name} is frozen\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUsPY5ucTSUT",
        "outputId": "bf5445e5-942c-40ce-c934-85e1e0e690bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj73fi47zpOE",
        "outputId": "60ca24e7-c5ba-40ed-d8da-362311d9818f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from conch.open_clip_custom import create_model_from_pretrained\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "# Load the Conch model\n",
        "def load_model(hf_token):\n",
        "    encoder_model, encoder_preprocess = create_model_from_pretrained('conch_ViT-B-16', \"hf_hub:MahmoodLab/conch\", hf_auth_token=hf_token)\n",
        "    return encoder_model, encoder_preprocess\n",
        "\n",
        "def image_to_embedding(encoder_model, encoder_preprocess, image):\n",
        "    # Assume image is a PIL.Image.Image object\n",
        "    image = encoder_preprocess(image).unsqueeze(0)\n",
        "    with torch.inference_mode():\n",
        "        image_embs = encoder_model.encode_image(image, proj_contrast=False, normalize=False)\n",
        "    return image_embs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def add_positional_encoding(tensor, output_shape=(16, 32)):\n",
        "    \"\"\"\n",
        "    Add positional encoding to a flattened feature vector and reshape it.\n",
        "\n",
        "    Args:\n",
        "        tensor: A torch.Tensor of shape [512].\n",
        "        output_shape: Tuple indicating the desired spatial dimensions (H, W).\n",
        "\n",
        "    Returns:\n",
        "        A tensor with positional encoding of shape [512, H, W].\n",
        "    \"\"\"\n",
        "    feature_dim = tensor.shape[1]\n",
        "    H, W = output_shape\n",
        "    assert feature_dim == H * W, \"Output shape must match the feature dimension.\"\n",
        "\n",
        "    # Reshape the input tensor to [1, H, W]\n",
        "    reshaped_tensor = tensor.view(1, H, W)\n",
        "\n",
        "    # Generate positional encoding (example: sinusoidal encoding)\n",
        "    x_pos = torch.arange(H).unsqueeze(1).repeat(1, W)  # H x W\n",
        "    y_pos = torch.arange(W).unsqueeze(0).repeat(H, 1)  # H x W\n",
        "    pos_enc = torch.stack([x_pos, y_pos], dim=0).float()  # 2 x H x W\n",
        "\n",
        "    # Normalize positional encodings (optional)\n",
        "    pos_enc = pos_enc / torch.sqrt(torch.tensor(H * W).float())\n",
        "\n",
        "    # Expand positional encodings to match the feature dimension\n",
        "    pos_enc = pos_enc.repeat(feature_dim // 2, 1, 1)  # [512, H, W]\n",
        "\n",
        "    # Add positional encoding to the reshaped tensor\n",
        "    encoded_tensor = reshaped_tensor + pos_enc[:reshaped_tensor.shape[0]]\n",
        "    return encoded_tensor\n"
      ],
      "metadata": {
        "id": "82dY3NFMb5Cz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2bwOyihFzpOE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define paths (update these paths based on where you extracted the files)\n",
        "image_folder = os.path.join(\"/content/Huron_data/\", \"Sliced_Images\")\n",
        "mask_folder = os.path.join(\"/content/Huron_data/\", \"Sliced_masks\")\n",
        "\n",
        "\n",
        "# Get sorted lists of image and mask files\n",
        "image_files = [os.path.join(image_folder, f) for f in sorted(os.listdir(image_folder))]\n",
        "mask_files = [os.path.join(mask_folder, f) for f in sorted(os.listdir(mask_folder))]\n",
        "\n",
        "# Ensure matching number of images and masks\n",
        "assert len(image_files) == len(mask_files), \"Mismatch between image and mask files.\"\n",
        "\n",
        "def crop_black_borders(image, threshold=30):\n",
        "    \"\"\"Crop black borders from an image based on a threshold for black pixels.\"\"\"\n",
        "    img_array = np.array(image)\n",
        "    gray_img = np.mean(img_array, axis=2)  # Convert to grayscale by averaging channels\n",
        "\n",
        "    # Initialize cropping boundaries\n",
        "    top, bottom = 0, gray_img.shape[0]\n",
        "    left, right = 0, gray_img.shape[1]\n",
        "\n",
        "    # Crop from the top\n",
        "    while top < bottom and np.mean(gray_img[top, :]) <= threshold:\n",
        "        top += 1\n",
        "\n",
        "    # Crop from the bottom\n",
        "    while bottom > top and np.mean(gray_img[bottom - 1, :]) <= threshold:\n",
        "        bottom -= 1\n",
        "\n",
        "    # Crop from the left\n",
        "    while left < right and np.mean(gray_img[:, left]) <= threshold:\n",
        "        left += 1\n",
        "\n",
        "    # Crop from the right\n",
        "    while right > left and np.mean(gray_img[:, right - 1]) <= threshold:\n",
        "        right -= 1\n",
        "\n",
        "    # Crop the image to the calculated bounds\n",
        "    cropped_image = image.crop((left, top, right, bottom))\n",
        "    return cropped_image\n",
        "\n",
        "def gaussian_blur(image, radius=2):\n",
        "    \"\"\"Apply Gaussian blur to an image to avoid capturing noise as tissue.\"\"\"\n",
        "    return image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
        "\n",
        "def preprocess_image(image_path, target_size=(128, 128)):\n",
        "    \"\"\"Preprocess an image: crop black borders, enhance contrast, and resize.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    cropped_image = crop_black_borders(image)\n",
        "\n",
        "    # blur to remove noise\n",
        "    blurred_image = gaussian_blur(cropped_image, 2) # increase radius if desire more blur(large neighborhood)\n",
        "\n",
        "    # Resize to the target size\n",
        "    resized_image = blurred_image.resize(target_size, Image.BICUBIC)\n",
        "\n",
        "    return resized_image\n",
        "\n",
        "\n",
        "def preprocess_mask(mask_path, target_size=(128, 128)):\n",
        "    \"\"\"Convert mask to binary, ensure tissue is white and background is black, and resize.\"\"\"\n",
        "    mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
        "    mask_array = np.array(mask)\n",
        "\n",
        "\n",
        "    # Apply binary threshold and ensure tissue is white, background is black\n",
        "    binary_mask = np.where(mask_array > 0, 1, 0).astype(np.uint8)  # Normalize mask to [0, 1]\n",
        "    binary_mask = Image.fromarray(binary_mask * 255)  # Convert back to PIL Image\n",
        "\n",
        "    # Resize to the target size using nearest-neighbor interpolation\n",
        "    resized_mask = binary_mask.resize(target_size, Image.NEAREST)\n",
        "\n",
        "    return resized_mask\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transforms for images and masks\n",
        "image_transform = transforms.Compose([\n",
        "\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "WMa2-XwRzpOE"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Dataset class\n",
        "class TissueDataset(Dataset):\n",
        "    def __init__(self, image_files, mask_files, image_processor=None, mask_transform=None, encoder_model=None, encoder_preprocess=None):\n",
        "        \"\"\"\n",
        "        Initialize the TissueDataset.\n",
        "\n",
        "        Parameters:\n",
        "        - image_files: List of image file paths.\n",
        "        - mask_files: List of mask file paths.\n",
        "        - image_processor: Preprocessing function for images (expects PIL input).\n",
        "        - mask_transform: Preprocessing function for masks.\n",
        "        \"\"\"\n",
        "        self.image_files = image_files\n",
        "        self.mask_files = mask_files\n",
        "        self.image_processor = image_processor\n",
        "        self.mask_transform = mask_transform\n",
        "        self.encoder_model = encoder_model\n",
        "        self.encoder_preprocess = encoder_preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask as PIL images\n",
        "        image = Image.open(self.image_files[idx]).convert(\"RGB\")  # Convert to PIL RGB\n",
        "        mask = Image.open(self.mask_files[idx]).convert(\"L\")  # Convert to PIL Grayscale\n",
        "\n",
        "        # process images\n",
        "        image = preprocess_image(self.image_files[idx])\n",
        "\n",
        "        # Process image using the image processor\n",
        "        if self.image_processor:\n",
        "            # Convert PIL image to tensor using the image processor\n",
        "            image = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        # Convert tensor back to PIL image\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            image = to_pil_image(image)\n",
        "\n",
        "        # Pass the processed image to the encoder\n",
        "        image = image_to_embedding(self.encoder_model, self.encoder_preprocess, image)\n",
        "\n",
        "        # Add positional encoding\n",
        "        image = add_positional_encoding(image)\n",
        "\n",
        "        # Process the mask\n",
        "        mask = preprocess_mask(self.mask_files[idx])\n",
        "\n",
        "        # Process the mask using mask_transform (if provided)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkkFAh90zpOE",
        "outputId": "dd85a6e7-be07-4000-d5b5-8c9f552f0178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/CONCH/conch/open_clip_custom/factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
          ]
        }
      ],
      "source": [
        "encoder_model, encoder_preprocess = load_model(HF_TOKEN)\n",
        "\n",
        "# Create dataset\n",
        "dataset = TissueDataset(\n",
        "    image_files=image_files,\n",
        "    mask_files=mask_files,\n",
        "    image_processor=image_processor,  # Pass the image processor here\n",
        "    mask_transform=mask_transform,     # Pass the mask transform here\n",
        "    encoder_model = encoder_model,\n",
        "    encoder_preprocess = encoder_preprocess\n",
        ")\n",
        "\n",
        "# Cut dataset for testing (optional for quick parameter testing)\n",
        "# dataset = torch.utils.data.Subset(dataset, range(0, len(dataset)//100))\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVilVRy4zpOF",
        "outputId": "15f5492d-0e2d-45b0-8fa4-cc69e9a86a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13877\n",
            "3470\n"
          ]
        }
      ],
      "source": [
        "# Print length of train and val dataset to verify split (if dataset was cut)\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image, mask = train_dataset[0]\n",
        "\n",
        "for image, mask in train_loader:\n",
        "  print(image.shape)\n",
        "  print(mask.shape)\n",
        "  break\n",
        "\n",
        "# print(image.shape)\n",
        "# print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pCN-7teXZUMv",
        "outputId": "3f0a715b-2b4b-42ac-ba9e-9d8e0e338a38"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 16, 32])\n",
            "torch.Size([16, 1, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(image_files[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeptT_8RZv0s",
        "outputId": "2a4a2edf-074e-4963-a23b-b49aa7cbfac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Huron_data/Sliced_Images/1.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset2 = TissueDataset(\n",
        "#     image_files=[image_files[1]],\n",
        "#     mask_files=[mask_files[1]],\n",
        "#     image_processor=image_processor,  # Pass the image processor here\n",
        "#     mask_transform=mask_transform,     # Pass the mask transform here\n",
        "#     encoder_model = encoder_model,\n",
        "#     encoder_preprocess = encoder_preprocess\n",
        "# )"
      ],
      "metadata": {
        "id": "pyi1TT8LZ64L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image, mask = dataset2[0]\n",
        "\n",
        "# print(image)\n",
        "# print(mask)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlBeAcctZ9Ik",
        "outputId": "c0da33f4-3eb7-4c92-8513-d06d922d8250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.7330e+00,  2.8707e-02,  1.7751e-01,  8.7870e-01,  2.3456e-02,\n",
            "          1.0810e+00, -1.7077e+00,  6.6926e-01,  4.8501e-01,  7.8096e-01,\n",
            "          3.5672e-01, -2.7618e-01,  7.5917e-01, -8.4548e-01,  1.0544e+00,\n",
            "          1.9687e+00,  1.2403e+00, -3.4095e-01,  7.9624e-01, -1.4819e+00,\n",
            "          3.6098e-01, -2.5850e-01, -1.6981e+00, -8.8867e-01,  1.3780e-01,\n",
            "          3.8530e-01,  2.3617e-01, -9.6151e-01,  1.2750e+00,  8.5740e-01,\n",
            "         -1.9091e+00, -6.9879e-01,  5.0598e-01,  1.0300e-01, -7.0285e-01,\n",
            "          2.0236e-01, -1.4430e+00,  2.2517e-01,  2.1445e-01,  1.2439e+00,\n",
            "         -1.0874e+00,  1.3317e-01,  3.1977e-01, -6.9955e-02,  3.4865e-02,\n",
            "          3.1697e-03,  3.5617e-02,  1.2634e+00,  5.2999e-01, -1.3280e+00,\n",
            "          6.3872e-01, -1.4391e+00, -9.4315e-01, -8.9843e-01, -8.6014e-01,\n",
            "          4.9759e-01, -1.1188e+00,  8.8039e-01,  9.0689e-01, -9.5366e-01,\n",
            "         -5.2227e-01,  1.8008e+00,  9.6926e-01,  1.1626e+00,  4.2102e-02,\n",
            "         -1.2774e-03, -5.8789e-01, -1.0474e+00, -8.6308e-01,  1.6192e+00,\n",
            "         -4.2394e-01, -1.1829e+00, -9.9720e-01,  5.5465e-01,  1.0520e+00,\n",
            "         -3.3343e-01, -2.0137e+00, -2.9121e+00, -1.8047e+00,  7.3258e-02,\n",
            "         -4.2652e-01,  2.6637e-01,  8.3898e-02,  2.7797e-01,  1.4486e+00,\n",
            "         -8.4853e-02, -2.0639e-02, -9.9787e-01,  1.1406e-01,  1.4939e+00,\n",
            "          4.5166e-01, -8.0111e-01, -2.0092e-01, -1.4127e+00, -8.0116e-02,\n",
            "          4.1817e-02, -8.7572e-01,  7.6677e-01, -1.1634e+00, -1.1115e+00,\n",
            "         -8.2955e-01, -1.3083e+00,  4.4706e-02, -6.9229e-01, -4.5093e-01,\n",
            "         -9.2325e-01,  7.1338e-01,  6.4749e-02,  9.4036e-01, -8.8817e-01,\n",
            "          4.2551e-02,  1.1975e+00,  8.4731e-01,  1.0127e+00, -3.4340e-01,\n",
            "          1.2421e-01, -1.5716e+00,  3.2839e-01, -9.0279e-01, -2.8667e+00,\n",
            "          9.3375e-01, -1.7103e-01, -2.6708e-01, -9.2465e-01,  1.0258e+00,\n",
            "          1.4016e+00,  5.6924e-01, -1.2818e-01,  5.2495e-01,  7.6976e-01,\n",
            "          1.7096e+00, -2.1630e+00,  1.4540e+00, -9.8933e-01,  4.3172e-01,\n",
            "         -8.8081e-02,  1.3953e+00,  6.3308e-01, -5.7093e-01,  2.4066e-01,\n",
            "         -3.2398e-01,  5.1076e-01,  1.0387e+00, -6.4974e-01, -1.3454e+00,\n",
            "         -1.6052e+00, -1.1327e+00,  4.1173e-01, -5.1226e-01, -1.4931e+00,\n",
            "          6.6458e-01,  4.7084e-01,  9.1832e-01, -4.5287e-01, -8.2511e-02,\n",
            "         -5.1467e-02,  4.3216e-01,  1.3858e+00,  2.2247e+00, -9.4542e-01,\n",
            "         -1.6920e-01, -6.5000e-01, -5.5954e-01, -1.4450e+00, -2.1473e-01,\n",
            "          4.4686e-01, -1.6354e+00,  1.1210e+00, -1.5394e-01,  9.4928e-01,\n",
            "          1.3325e+00,  2.0184e+00, -2.2532e+00, -2.4460e+00,  1.2708e+00,\n",
            "          4.4300e-01,  7.7852e-02, -3.0407e-01,  8.0479e-01,  7.8659e-01,\n",
            "          4.3039e-01, -7.5978e-01, -4.0724e-01,  3.1069e-01,  1.3247e+00,\n",
            "         -2.6397e-01,  2.3330e-01, -9.8223e-01, -1.0085e+00,  1.7255e+00,\n",
            "          1.1721e+00,  6.1146e-01,  6.3069e-01,  3.7980e-01,  2.3901e-02,\n",
            "         -1.3782e+00, -6.6949e-01, -1.2372e+00, -1.0617e+00, -1.3465e+00,\n",
            "         -1.9691e+00,  1.3038e+00,  7.8247e-01,  2.5629e+00, -7.6642e-02,\n",
            "          5.7995e-01, -1.0943e+00,  7.0356e-01,  3.1733e-01, -1.4105e+00,\n",
            "         -4.3649e-01,  1.1725e+00,  7.2595e-01,  6.0187e-01,  8.0695e-01,\n",
            "          4.1863e-01,  9.9506e-01, -3.9245e-01, -4.9478e-01,  1.1068e+00,\n",
            "         -9.0829e-01,  2.6051e-01,  7.3660e-01,  6.6290e-01,  1.1079e-01,\n",
            "          1.2698e+00,  1.6005e-01,  3.0323e-01, -9.3155e-01,  1.7846e+00,\n",
            "          1.4111e+00,  1.2382e+00,  1.2049e+00,  1.0467e+00, -6.9863e-01,\n",
            "         -2.6249e-01,  1.1227e+00, -1.0616e+00,  1.8802e+00,  2.6654e-01,\n",
            "          7.5112e-01, -3.5281e-01,  4.7334e-02, -5.6788e-01,  1.2018e+00,\n",
            "         -1.2414e+00,  1.9586e+00, -1.4066e+00,  1.4741e+00, -3.6319e-01,\n",
            "          4.3425e-01,  9.0653e-01,  6.0240e-01, -2.5279e-01,  1.2148e+00,\n",
            "          7.8279e-01,  1.4243e+00,  6.8821e-01,  2.4495e-01,  3.4893e-01,\n",
            "          1.4338e+00,  6.0998e-01, -1.0733e+00, -9.9482e-01,  1.1317e+00,\n",
            "          2.6826e-01, -9.0210e-01,  2.8004e-01, -1.1905e+00,  1.0636e+00,\n",
            "          1.0872e+00, -5.3744e-01,  1.0737e+00, -5.9659e-01,  7.5294e-01,\n",
            "          4.1204e-01, -2.6444e-01,  4.0881e-01, -1.6356e+00, -5.0009e-01,\n",
            "         -1.5253e+00, -4.4630e-01,  1.0058e+00, -2.9322e+00, -3.2942e-01,\n",
            "          1.6001e+00, -7.0860e-01, -1.5239e+00, -1.4253e+00,  4.5781e-01,\n",
            "          1.0530e+00,  3.0908e-01,  1.0962e-01, -2.7859e-01,  2.0184e-01,\n",
            "          9.2599e-01,  1.2688e+00, -4.4359e-01,  5.8100e-01,  1.8457e-01,\n",
            "         -7.3981e-01, -6.8487e-01,  3.5889e-01, -6.9693e-01,  7.8724e-01,\n",
            "          2.5838e-01,  9.6304e-01, -8.7019e-01, -9.8250e-01, -6.9000e-02,\n",
            "          1.7521e+00,  6.4251e-01, -1.2362e-01,  6.3559e-02,  1.0903e+00,\n",
            "          7.9020e-02,  1.2250e+00,  6.3767e-01, -2.8335e-01, -1.7833e+00,\n",
            "          1.4125e+00, -2.8620e-01, -1.0496e+00, -1.1271e-01,  6.8166e-01,\n",
            "         -5.7449e-01,  6.9651e-01,  1.0653e+00,  1.5944e+00,  1.0558e+00,\n",
            "         -7.0100e-01,  1.0209e+00, -2.5006e+00,  8.9120e-01,  3.0176e-01,\n",
            "         -1.3159e+00,  6.3011e-01,  2.8612e-01,  1.1484e-01,  2.9154e-02,\n",
            "         -2.4115e+00,  1.3927e+00,  1.1793e+00,  2.5822e-01, -1.0394e+00,\n",
            "          1.3690e+00,  1.1459e+00, -1.2168e+00, -5.7707e-01,  6.2782e-03,\n",
            "         -5.3553e-01, -6.3295e-01,  1.4512e-01, -7.3624e-01,  1.0564e+00,\n",
            "          9.1674e-01,  1.7512e-02,  1.2571e+00,  3.5539e-01, -1.9007e+00,\n",
            "          4.2964e-01, -7.8302e-01,  1.7712e-01, -4.7279e-01, -1.2397e+00,\n",
            "          3.1801e-01,  8.5049e-01,  9.0405e-01,  2.2989e-01, -1.5401e+00,\n",
            "          5.6857e-01,  1.2258e-01,  9.3090e-01, -4.7988e-01,  1.5217e+00,\n",
            "         -1.7034e+00,  2.7218e-01, -2.7812e-01,  6.3176e-01,  1.1470e+00,\n",
            "          5.7987e-01, -1.5560e-01,  6.2647e-01, -1.7855e+00, -1.2028e+00,\n",
            "         -5.9027e-01, -1.3473e+00,  1.5825e-01,  2.0473e-02,  1.0345e+00,\n",
            "         -1.2030e+00,  8.8137e-01, -4.8284e-01,  2.1554e+00, -8.4456e-01,\n",
            "          6.0350e-01, -5.3076e-01,  7.6279e-01,  5.7081e-01,  7.3795e-01,\n",
            "          8.9683e-01, -3.4361e-01, -9.0022e-01, -1.8939e+00, -1.3234e+00,\n",
            "          9.6787e-01,  6.0271e-01,  2.9064e-01, -4.0630e-01,  4.1359e-01,\n",
            "         -3.1977e-01, -1.0673e+00, -2.6954e-01,  2.2365e+00,  4.3618e-02,\n",
            "          1.5192e+00, -5.2571e-01,  4.1818e-02, -9.5395e-01,  2.9255e-01,\n",
            "          4.3594e-01,  1.9963e-01, -1.0098e+00,  4.6214e-01,  7.3340e-01,\n",
            "         -1.1800e+00, -2.7428e-01, -8.6729e-01, -8.7878e-01,  9.1974e-01,\n",
            "         -3.7751e-01,  3.9047e-01, -1.3099e+00,  3.8426e-01,  5.1635e-01,\n",
            "         -1.0190e+00,  6.2760e-01,  3.6635e-02,  6.2045e-02, -1.5458e+00,\n",
            "         -2.5188e+00,  1.5554e+00,  6.4822e-01,  2.6589e-01,  1.3129e-01,\n",
            "         -1.1098e+00,  5.3841e-01, -1.4257e-01, -5.1998e-01, -1.8435e+00,\n",
            "         -1.0852e+00,  9.6989e-02,  2.2835e+00,  1.3075e+00,  9.2524e-01,\n",
            "          1.1978e+00, -4.2991e-01,  1.3624e+00, -1.4427e+00,  6.3132e-01,\n",
            "          2.7133e-01,  1.5636e+00, -1.3491e+00, -3.4164e-01, -1.3436e+00,\n",
            "         -1.2636e+00, -7.6875e-02, -2.4011e+00,  1.1142e+00, -7.7629e-01,\n",
            "         -9.0893e-01, -4.1008e-01,  5.5833e-01,  1.0354e+00, -9.7356e-01,\n",
            "         -4.1402e-01, -3.1593e-01,  9.2428e-01, -3.8759e-01,  2.3038e-01,\n",
            "          1.0958e-01,  7.2626e-02, -1.9800e-01,  9.7407e-02, -2.8846e+00,\n",
            "         -8.0026e-01,  2.2074e+00,  1.5909e-01,  5.0426e-01,  3.2000e+00,\n",
            "         -6.5394e-01,  1.1438e-01, -1.2802e+00, -4.2196e-02, -7.6558e-01,\n",
            "         -9.3837e-01, -1.0622e+00, -1.7898e-02, -1.3446e+00, -2.5777e-01,\n",
            "         -1.1258e+00, -6.5825e-01,  4.4382e-01,  9.6420e-01,  7.0524e-02,\n",
            "         -1.7665e+00,  3.9582e-01, -7.5949e-02, -1.2303e+00, -9.1847e-01,\n",
            "         -6.2399e-01, -2.2686e-01]])\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pk5TR5p5zpOF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        # Apply sigmoid to inputs if not already done\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        # Flatten\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "        return 1 - dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wrXgZQ0dzpOG"
      },
      "outputs": [],
      "source": [
        "def calculate_iou_infer(predicted_mask, ground_truth_mask):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) between the predicted mask and the ground truth mask.\n",
        "\n",
        "    Args:\n",
        "        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n",
        "        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        float: IoU score.\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n",
        "    union = np.logical_or(predicted_mask, ground_truth_mask).sum()\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def calculate_dice_infer(predicted_mask, ground_truth_mask):\n",
        "    \"\"\"\n",
        "    Calculate Dice Coefficient between the predicted mask and the ground truth mask.\n",
        "\n",
        "    Args:\n",
        "        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n",
        "        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        float: Dice Coefficient score.\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n",
        "    return (2 * intersection) / (predicted_mask.sum() + ground_truth_mask.sum()) if (predicted_mask.sum() + ground_truth_mask.sum()) > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SvM6TKiTzpOG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# combined dice and BCE loss function\n",
        "# Define Combined Dice and BCE Loss\n",
        "class CombinedDiceBCELoss(nn.Module):\n",
        "    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth=1e-6):\n",
        "        super(CombinedDiceBCELoss, self).__init__()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce_weight = bce_weight\n",
        "        self.smooth = smooth\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # Dice Loss\n",
        "        probs = torch.sigmoid(logits)\n",
        "        intersection = (probs * targets).sum(dim=(1, 2))  # Sum over spatial dimensions only\n",
        "        dice_loss = 1 - (2. * intersection + self.smooth) / (probs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) + self.smooth)\n",
        "        dice_loss = dice_loss.mean()  # Average over the batch\n",
        "\n",
        "        # BCE Loss\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "\n",
        "        # Combine losses\n",
        "        return self.dice_weight * dice_loss + self.bce_weight * bce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH4OGt99zpOG",
        "outputId": "bd39bb0b-0115-4255-f74f-29253f934a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# print the device\n",
        "print(device)\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Set a smaller learning rate for fine-tuning\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "# Assuming DiceLoss is already defined in your environment\n",
        "criterion = DiceLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    \"\"\"\n",
        "    Training function with loss calculation.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for pixel_values, masks in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\"):\n",
        "            # Move inputs and masks to the correct device\n",
        "            pixel_values = pixel_values.to(device, dtype=next(model.parameters()).dtype)  # Match model dtype\n",
        "            masks = masks.to(device, dtype=torch.float32)  # Ensure masks are on the correct device and float32\n",
        "\n",
        "            if pixel_values.shape[1] == 1:  # Check if channels are 1\n",
        "              pixel_values = pixel_values.repeat(1, 3, 1, 1)  # Repeat across channel dimension to make it [batch_size, 3, H, W]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n",
        "\n",
        "            # Resize logits to match masks\n",
        "            tissue_logits_resized = F.interpolate(\n",
        "                tissue_logits.unsqueeze(1),  # Add channel dimension\n",
        "                size=masks.shape[-2:],  # Match mask size\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False\n",
        "            ).squeeze(1)  # Remove channel dimension\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(tissue_logits_resized, masks)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validation function aligned with inference logic, including IoU and Dice metric calculation.\n",
        "\n",
        "    Args:\n",
        "        model: The trained segmentation model.\n",
        "        val_loader: DataLoader providing validation images and ground truth masks.\n",
        "        criterion: Loss function for evaluation.\n",
        "        device: Computation device (CPU or CUDA).\n",
        "\n",
        "    Returns:\n",
        "        avg_val_loss: Average validation loss.\n",
        "        avg_iou: Average IoU across the validation set.\n",
        "        avg_dice: Average Dice score across the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_dice = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, ground_truth_masks in tqdm(val_loader, desc=\"Validation\"):\n",
        "            # Move ground truth masks to device and convert to float\n",
        "            ground_truth_masks = ground_truth_masks.to(device, dtype=torch.float32)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(pixel_values=images.to(device))\n",
        "            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n",
        "\n",
        "            # Resize logits to match ground truth mask size\n",
        "            tissue_logits_resized = torch.sigmoid(F.interpolate(\n",
        "                tissue_logits.unsqueeze(1),  # Add channel dimension\n",
        "                size=ground_truth_masks.shape[-2:],  # Match mask size\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False\n",
        "            ).squeeze(1))  # Remove channel dimension\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(tissue_logits_resized, ground_truth_masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Convert predicted logits to binary masks\n",
        "            predicted_masks = (tissue_logits_resized > 0.5).cpu().numpy().astype(np.uint8)\n",
        "            ground_truth_masks_np = ground_truth_masks.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "            # Compute metrics\n",
        "            for pred, gt in zip(predicted_masks, ground_truth_masks_np):\n",
        "                total_iou += calculate_iou_infer(pred, gt)\n",
        "                total_dice += calculate_dice_infer(pred, gt)\n",
        "                num_samples += 1\n",
        "\n",
        "    # Calculate average loss and metrics\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_iou = total_iou / num_samples if num_samples > 0 else 0\n",
        "    avg_dice = total_dice / num_samples if num_samples > 0 else 0\n",
        "\n",
        "    print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Mean IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Mean Dice: {avg_dice:.4f}\")\n",
        "\n",
        "    return avg_val_loss, avg_iou, avg_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGY6IUotzpOG",
        "outputId": "6dc09a07-4ea5-40f8-82eb-65ecdcc21da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/1] Training: 100%|██████████| 868/868 [1:32:42<00:00,  6.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.5394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, criterion, optimizer, num_epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/trained_encoded.pth')"
      ],
      "metadata": {
        "id": "NDPotb8qu0j1"
      },
      "execution_count": 49,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a08567b369c41a48a430a710bf787c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f2f8cbf2a774ea59db3b520da83637e",
              "IPY_MODEL_3d434016e9b3462aba41852c643212e9",
              "IPY_MODEL_5e2e5108da174d57943482a6d5a05801"
            ],
            "layout": "IPY_MODEL_1af7289de1034fc78a60e9411ca9a54b"
          }
        },
        "8f2f8cbf2a774ea59db3b520da83637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f49716a162ed4ad2ae6db69f3bef0a4c",
            "placeholder": "​",
            "style": "IPY_MODEL_68b9fbab27f444e28436868a57ef8cc6",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "3d434016e9b3462aba41852c643212e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54539bf928b74db1b6d8bd109dd21887",
            "max": 538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_280747e87a6e4e768e6c5b1c595b6e27",
            "value": 538
          }
        },
        "5e2e5108da174d57943482a6d5a05801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c97cf33eed40168066de58083e91d3",
            "placeholder": "​",
            "style": "IPY_MODEL_fbeea4c2168746c9aea4832b0777aaa4",
            "value": " 538/538 [00:00&lt;00:00, 48.4kB/s]"
          }
        },
        "1af7289de1034fc78a60e9411ca9a54b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49716a162ed4ad2ae6db69f3bef0a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b9fbab27f444e28436868a57ef8cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54539bf928b74db1b6d8bd109dd21887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280747e87a6e4e768e6c5b1c595b6e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54c97cf33eed40168066de58083e91d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbeea4c2168746c9aea4832b0777aaa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f91538a846f3428890481241acf482d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fcd5741f1da4a3899b266889bdfac05",
              "IPY_MODEL_1821cf23d0444e34aa95803babcb5873",
              "IPY_MODEL_285db2e63ec54eb2be7c40d2360fd288"
            ],
            "layout": "IPY_MODEL_384bbb1c26c94ed98f59bdbb2add761a"
          }
        },
        "2fcd5741f1da4a3899b266889bdfac05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445add1a257d43678cf9210ca5ae5085",
            "placeholder": "​",
            "style": "IPY_MODEL_308038273f1e4f5598828b20e809d417",
            "value": "config.json: 100%"
          }
        },
        "1821cf23d0444e34aa95803babcb5873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dd3bc2f675e481d855ec355f89e3019",
            "max": 9470,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b60504b1f05e41768c63c36a99cfddb6",
            "value": 9470
          }
        },
        "285db2e63ec54eb2be7c40d2360fd288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe63d4ca7eb24dcaa6c1b5f4e8df9655",
            "placeholder": "​",
            "style": "IPY_MODEL_fa9ef4b0104b4b6ab4292aa24c24a831",
            "value": " 9.47k/9.47k [00:00&lt;00:00, 767kB/s]"
          }
        },
        "384bbb1c26c94ed98f59bdbb2add761a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445add1a257d43678cf9210ca5ae5085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "308038273f1e4f5598828b20e809d417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dd3bc2f675e481d855ec355f89e3019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60504b1f05e41768c63c36a99cfddb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe63d4ca7eb24dcaa6c1b5f4e8df9655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9ef4b0104b4b6ab4292aa24c24a831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d468821243a349bf970a109108da9cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e00aa9a539444e9591d2fbd24a9debd9",
              "IPY_MODEL_8ef664a821f840199f344229197db753",
              "IPY_MODEL_be2a5bed085d457f83f77885785122b1"
            ],
            "layout": "IPY_MODEL_529382db43904f84a30d1ba4f46c410d"
          }
        },
        "e00aa9a539444e9591d2fbd24a9debd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_693d31724d2e48f3930b8fb9c4862da9",
            "placeholder": "​",
            "style": "IPY_MODEL_0ecc269b77a1462e9df0066c64fd80b2",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8ef664a821f840199f344229197db753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e712b0cc27443368ed72fc021d8a31d",
            "max": 431950789,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94b77d537ec9480eaa339f0bffb745d9",
            "value": 431950789
          }
        },
        "be2a5bed085d457f83f77885785122b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2079483623214bc7b8afbb2d467a56fd",
            "placeholder": "​",
            "style": "IPY_MODEL_85a59e8b121b4f41a2900a27f91cc51a",
            "value": " 432M/432M [00:02&lt;00:00, 228MB/s]"
          }
        },
        "529382db43904f84a30d1ba4f46c410d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693d31724d2e48f3930b8fb9c4862da9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecc269b77a1462e9df0066c64fd80b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e712b0cc27443368ed72fc021d8a31d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b77d537ec9480eaa339f0bffb745d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2079483623214bc7b8afbb2d467a56fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85a59e8b121b4f41a2900a27f91cc51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}