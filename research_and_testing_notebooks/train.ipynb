{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21737,"status":"ok","timestamp":1732216221427,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"N7qwnx_0SvV5","outputId":"a18e7ba2-79a0-48cd-aedd-28dedd45f6ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1037,"status":"ok","timestamp":1732199408973,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"uZxAjfaWT5ki","outputId":"8e5b1d60-cba1-40a3-a5bb-648999b5f3af"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Contacts.vcf', 'Innovation Proposal Research Topic.gdoc', 'Innovation_Proposal_Presentation_Slides.gslides', 'Library_Research_Report_Brandon_Leblanc.pdf', 'Tutorial_Proposal_Outline.gdoc', \"Innovation_Proposal_Brandon-Leblanc-40058666_Karim-Hanna-40245600_Adriano-D'Ermo-40246394_Alessandro-Antonacci-4024560.gdoc\", 'Colab Notebooks', 'COMP433_Lab2_Ex.ipynb', 'Untitled document.gdoc', 'COMP433_proposal.gdoc', 'mask2former_tissue_segmentation_epoch5_batch8']\n"]}],"source":["import os\n","\n","# List the contents of My Drive\n","drive_path = '/content/drive/My Drive/'\n","files = os.listdir(drive_path)\n","print(files)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_al5TeADSvBg","executionInfo":{"status":"ok","timestamp":1732216325927,"user_tz":300,"elapsed":88057,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["import zipfile\n","\n","# Path to the ZIP file in Google Drive\n","zip_file_path = '/content/drive/My Drive/Colab Notebooks/data.zip'\n","\n","\n","# Extract the ZIP file\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall('/content/Huron_data')"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3176,"status":"ok","timestamp":1732230323798,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"7UZvV6GXVBys","outputId":"dbff8b8d-ed8e-433f-a0d3-56369fabe6b9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n","  return func(*args, **kwargs)\n","Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-ade-semantic and are newly initialized because the shapes did not match:\n","- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n","- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n","- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight is frozen\n","model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias is frozen\n","model.pixel_level_module.encoder.embeddings.norm.weight is frozen\n","model.pixel_level_module.encoder.embeddings.norm.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight is frozen\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage1.weight is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage1.bias is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage2.weight is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage2.bias is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage3.weight is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage3.bias is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage4.weight is frozen\n","model.pixel_level_module.encoder.hidden_states_norms.stage4.bias is frozen\n","model.pixel_level_module.decoder.level_embed is trainable\n","model.pixel_level_module.decoder.input_projections.0.0.weight is trainable\n","model.pixel_level_module.decoder.input_projections.0.0.bias is trainable\n","model.pixel_level_module.decoder.input_projections.0.1.weight is trainable\n","model.pixel_level_module.decoder.input_projections.0.1.bias is trainable\n","model.pixel_level_module.decoder.input_projections.1.0.weight is trainable\n","model.pixel_level_module.decoder.input_projections.1.0.bias is trainable\n","model.pixel_level_module.decoder.input_projections.1.1.weight is trainable\n","model.pixel_level_module.decoder.input_projections.1.1.bias is trainable\n","model.pixel_level_module.decoder.input_projections.2.0.weight is trainable\n","model.pixel_level_module.decoder.input_projections.2.0.bias is trainable\n","model.pixel_level_module.decoder.input_projections.2.1.weight is trainable\n","model.pixel_level_module.decoder.input_projections.2.1.bias is trainable\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.fc1.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.fc1.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.fc2.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.fc2.bias is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight is frozen\n","model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias is frozen\n","model.pixel_level_module.decoder.mask_projection.weight is trainable\n","model.pixel_level_module.decoder.mask_projection.bias is trainable\n","model.pixel_level_module.decoder.adapter_1.0.weight is trainable\n","model.pixel_level_module.decoder.adapter_1.1.weight is trainable\n","model.pixel_level_module.decoder.adapter_1.1.bias is trainable\n","model.pixel_level_module.decoder.layer_1.0.weight is trainable\n","model.pixel_level_module.decoder.layer_1.1.weight is trainable\n","model.pixel_level_module.decoder.layer_1.1.bias is trainable\n","model.transformer_module.queries_embedder.weight is trainable\n","model.transformer_module.queries_features.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.0.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.0.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.0.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.0.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.0.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.0.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.0.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.0.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.0.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.0.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.0.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.0.fc1.weight is trainable\n","model.transformer_module.decoder.layers.0.fc1.bias is trainable\n","model.transformer_module.decoder.layers.0.fc2.weight is trainable\n","model.transformer_module.decoder.layers.0.fc2.bias is trainable\n","model.transformer_module.decoder.layers.0.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.0.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.1.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.1.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.1.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.1.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.1.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.1.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.1.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.1.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.1.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.1.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.1.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.1.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.1.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.1.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.1.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.1.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.1.fc1.weight is trainable\n","model.transformer_module.decoder.layers.1.fc1.bias is trainable\n","model.transformer_module.decoder.layers.1.fc2.weight is trainable\n","model.transformer_module.decoder.layers.1.fc2.bias is trainable\n","model.transformer_module.decoder.layers.1.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.1.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.2.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.2.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.2.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.2.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.2.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.2.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.2.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.2.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.2.fc1.weight is trainable\n","model.transformer_module.decoder.layers.2.fc1.bias is trainable\n","model.transformer_module.decoder.layers.2.fc2.weight is trainable\n","model.transformer_module.decoder.layers.2.fc2.bias is trainable\n","model.transformer_module.decoder.layers.2.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.2.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.3.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.3.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.3.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.3.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.3.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.3.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.3.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.3.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.3.fc1.weight is trainable\n","model.transformer_module.decoder.layers.3.fc1.bias is trainable\n","model.transformer_module.decoder.layers.3.fc2.weight is trainable\n","model.transformer_module.decoder.layers.3.fc2.bias is trainable\n","model.transformer_module.decoder.layers.3.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.3.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.4.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.4.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.4.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.4.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.4.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.4.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.4.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.4.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.4.fc1.weight is trainable\n","model.transformer_module.decoder.layers.4.fc1.bias is trainable\n","model.transformer_module.decoder.layers.4.fc2.weight is trainable\n","model.transformer_module.decoder.layers.4.fc2.bias is trainable\n","model.transformer_module.decoder.layers.4.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.4.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.5.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.5.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.5.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.5.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.5.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.5.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.5.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.5.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.5.fc1.weight is trainable\n","model.transformer_module.decoder.layers.5.fc1.bias is trainable\n","model.transformer_module.decoder.layers.5.fc2.weight is trainable\n","model.transformer_module.decoder.layers.5.fc2.bias is trainable\n","model.transformer_module.decoder.layers.5.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.5.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.6.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.6.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.6.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.6.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.6.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.6.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.6.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.6.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.6.fc1.weight is trainable\n","model.transformer_module.decoder.layers.6.fc1.bias is trainable\n","model.transformer_module.decoder.layers.6.fc2.weight is trainable\n","model.transformer_module.decoder.layers.6.fc2.bias is trainable\n","model.transformer_module.decoder.layers.6.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.6.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.7.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.7.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.7.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.7.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.7.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.7.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.7.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.7.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.7.fc1.weight is trainable\n","model.transformer_module.decoder.layers.7.fc1.bias is trainable\n","model.transformer_module.decoder.layers.7.fc2.weight is trainable\n","model.transformer_module.decoder.layers.7.fc2.bias is trainable\n","model.transformer_module.decoder.layers.7.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.7.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.8.self_attn.k_proj.weight is trainable\n","model.transformer_module.decoder.layers.8.self_attn.k_proj.bias is trainable\n","model.transformer_module.decoder.layers.8.self_attn.v_proj.weight is trainable\n","model.transformer_module.decoder.layers.8.self_attn.v_proj.bias is trainable\n","model.transformer_module.decoder.layers.8.self_attn.q_proj.weight is trainable\n","model.transformer_module.decoder.layers.8.self_attn.q_proj.bias is trainable\n","model.transformer_module.decoder.layers.8.self_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.8.self_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight is trainable\n","model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias is trainable\n","model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight is trainable\n","model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias is trainable\n","model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias is trainable\n","model.transformer_module.decoder.layers.8.fc1.weight is trainable\n","model.transformer_module.decoder.layers.8.fc1.bias is trainable\n","model.transformer_module.decoder.layers.8.fc2.weight is trainable\n","model.transformer_module.decoder.layers.8.fc2.bias is trainable\n","model.transformer_module.decoder.layers.8.final_layer_norm.weight is trainable\n","model.transformer_module.decoder.layers.8.final_layer_norm.bias is trainable\n","model.transformer_module.decoder.layernorm.weight is trainable\n","model.transformer_module.decoder.layernorm.bias is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.weight is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.0.0.bias is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.weight is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.1.0.bias is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.weight is trainable\n","model.transformer_module.decoder.mask_predictor.mask_embedder.2.0.bias is trainable\n","model.transformer_module.level_embed.weight is trainable\n","class_predictor.weight is trainable\n","class_predictor.bias is trainable\n"]}],"source":["from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor\n","import torch\n","import gc\n","\n","def clear_model_and_cache():\n","    \"\"\"Utility function to delete existing model and optimizer objects and clear GPU memory.\"\"\"\n","    if 'model' in globals():\n","        print(\"Deleting existing model...\")\n","        del globals()['optimizer']\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# Clear any existing model and cache\n","\n","# clear_model_and_cache()\n","\n","# Load the image processor with relevant settings\n","image_processor = Mask2FormerImageProcessor.from_pretrained(\n","    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n","    do_rescale=True,   # Skip rescaling if images are already normalized\n","    do_normalize=False,  # DO NOT NORMALIZE. POOR RESULTS FOR BINARY SEGMENATATION.\n","    do_resize=True     # Skip resizing as we're handling this during preprocessing\n",")\n","\n","# Load the Mask2Former model for binary segmentation\n","model = Mask2FormerForUniversalSegmentation.from_pretrained(\n","    \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n","    num_labels=2,                     # Binary segmentation (background and tissue)\n","    ignore_mismatched_sizes=True      # Allow resizing of model parameters if dimensions do not match\n",")\n","\n","###############################################\n","# Freezing encoder backbone if desired\n","###############################################\n","\n","\n","# Freeze the backbone of the Mask2Former model\n","for name, param in model.named_parameters():\n","    if \"encoder\" in name:  # Match all layers within the encoder (backbone)\n","        param.requires_grad = False\n","\n","# Display the trainable layers for confirmation\n","# Print trainable layers\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"{name} is trainable\")\n","    else:\n","        print(f\"{name} is frozen\")"]},{"cell_type":"markdown","metadata":{"id":"dInkl8TqSuvz"},"source":[]},{"cell_type":"code","execution_count":97,"metadata":{"id":"AXx6-fdnF_Xe","executionInfo":{"status":"ok","timestamp":1732230369243,"user_tz":300,"elapsed":1020,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image, ImageFilter\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch\n","\n","\n","# Define paths (update these paths based on where you extracted the files)\n","image_folder = os.path.join(\"/content/Huron_data/data/Huron_data/\", \"Sliced_Images\")\n","mask_folder = os.path.join(\"/content/Huron_data/data/Huron_data/\", \"Sliced_masks\")\n","\n","\n","# Get sorted lists of image and mask files\n","image_files = [os.path.join(image_folder, f) for f in sorted(os.listdir(image_folder))]\n","mask_files = [os.path.join(mask_folder, f) for f in sorted(os.listdir(mask_folder))]\n","\n","# Ensure matching number of images and masks\n","assert len(image_files) == len(mask_files), \"Mismatch between image and mask files.\"\n","\n","def crop_black_borders(image, threshold=30):\n","    \"\"\"Crop black borders from an image based on a threshold for black pixels.\"\"\"\n","    img_array = np.array(image)\n","    gray_img = np.mean(img_array, axis=2)  # Convert to grayscale by averaging channels\n","\n","    # Initialize cropping boundaries\n","    top, bottom = 0, gray_img.shape[0]\n","    left, right = 0, gray_img.shape[1]\n","\n","    # Crop from the top\n","    while top < bottom and np.mean(gray_img[top, :]) <= threshold:\n","        top += 1\n","\n","    # Crop from the bottom\n","    while bottom > top and np.mean(gray_img[bottom - 1, :]) <= threshold:\n","        bottom -= 1\n","\n","    # Crop from the left\n","    while left < right and np.mean(gray_img[:, left]) <= threshold:\n","        left += 1\n","\n","    # Crop from the right\n","    while right > left and np.mean(gray_img[:, right - 1]) <= threshold:\n","        right -= 1\n","\n","    # Crop the image to the calculated bounds\n","    cropped_image = image.crop((left, top, right, bottom))\n","    return cropped_image\n","\n","def gaussian_blur(image, radius=2):\n","    \"\"\"Apply Gaussian blur to an image to avoid capturing noise as tissue.\"\"\"\n","    return image.filter(ImageFilter.GaussianBlur(radius=radius))\n","\n","def preprocess_image(image_path, target_size=(128, 128)):\n","    \"\"\"Preprocess an image: crop black borders, enhance contrast, and resize.\"\"\"\n","    image = Image.open(image_path).convert(\"RGB\")\n","    cropped_image = crop_black_borders(image)\n","\n","    # blur to remove noise\n","    blurred_image = gaussian_blur(cropped_image, 2) # increase radius if desire more blur(large neighborhood)\n","\n","    # Resize to the target size\n","    resized_image = blurred_image.resize(target_size, Image.BICUBIC)\n","\n","    return resized_image\n","\n","\n","def preprocess_mask(mask_path, target_size=(128, 128)):\n","    \"\"\"Convert mask to binary, ensure tissue is white and background is black, and resize.\"\"\"\n","    mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n","    mask_array = np.array(mask)\n","\n","\n","    # Apply binary threshold and ensure tissue is white, background is black\n","    binary_mask = np.where(mask_array > 0, 1, 0).astype(np.uint8)  # Normalize mask to [0, 1]\n","    binary_mask = Image.fromarray(binary_mask * 255)  # Convert back to PIL Image\n","\n","    # Resize to the target size using nearest-neighbor interpolation\n","    resized_mask = binary_mask.resize(target_size, Image.NEAREST)\n","\n","    return resized_mask\n","\n","from torchvision import transforms\n","\n","# Define transforms for images and masks\n","image_transform = transforms.Compose([\n","\n","    transforms.ToTensor(),  # Convert to tensor\n","])\n","\n","mask_transform = transforms.Compose([\n","\n","    transforms.ToTensor(),  # Convert to tensor\n","])\n","\n","# Dataset class\n","class TissueDataset(Dataset):\n","    def __init__(self, image_files, mask_files, image_processor=None, mask_transform=None):\n","        \"\"\"\n","        Initialize the TissueDataset.\n","\n","        Parameters:\n","        - image_files: List of image file paths.\n","        - mask_files: List of mask file paths.\n","        - image_processor: Preprocessing function for images (expects PIL input).\n","        - mask_transform: Preprocessing function for masks.\n","        \"\"\"\n","        self.image_files = image_files\n","        self.mask_files = mask_files\n","        self.image_processor = image_processor\n","        self.mask_transform = mask_transform\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image and mask as PIL images\n","        image = Image.open(self.image_files[idx]).convert(\"RGB\")  # Convert to PIL RGB\n","        mask = Image.open(self.mask_files[idx]).convert(\"L\")  # Convert to PIL Grayscale\n","        # process images\n","        image = preprocess_image(self.image_files[idx])\n","        mask = preprocess_mask(self.mask_files[idx])\n","        # Process image using the image processor\n","        if self.image_processor:\n","            # Convert PIL image to tensor using the image processor\n","            image = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n","\n","        # Process the mask using mask_transform (if provided)\n","        if self.mask_transform:\n","            mask = self.mask_transform(mask)\n","\n","        return image, mask\n","\n","# Create dataset\n","dataset = TissueDataset(\n","    image_files=image_files,\n","    mask_files=mask_files,\n","    image_processor=image_processor,  # Pass the image processor here\n","    mask_transform=mask_transform     # Pass the mask transform here\n",")\n","\n","# Cut dataset for testing (optional for quick parameter testing)\n","# dataset = torch.utils.data.Subset(dataset, range(0, len(dataset)//10))\n","\n","# Split dataset into train and validation sets\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1732230391570,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"3GDLwt75F_Xf","outputId":"aa5f99d5-4e96-47e7-b27d-954807bd5785"},"outputs":[{"output_type":"stream","name":"stdout","text":["13877\n","3470\n"]}],"source":["# Print length of train and val dataset to verify split (if dataset was cut)\n","print(len(train_dataset))\n","print(len(val_dataset))\n"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"108OJDAglk-ZewEgmvu5rW6cGDJcR5CEb"},"executionInfo":{"elapsed":15939,"status":"ok","timestamp":1732230409470,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"uFSN6VV2F_Xg","outputId":"524cad48-982c-428e-9c7d-02187f91f189"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","def visualize_batch_from_loader(loader, num_batches=1):\n","    \"\"\"Visualize a few batches of images and masks from the DataLoader.\"\"\"\n","    loader_iter = iter(loader)\n","\n","    for batch_idx in range(num_batches):\n","        # Get the next batch of images and masks\n","        images, masks = next(loader_iter)\n","\n","        # Move tensors to CPU if necessary and convert to numpy\n","        images_np = images.permute(0, 2, 3, 1).cpu().numpy()\n","        masks_np = masks.squeeze(1).cpu().numpy()  # Remove channel dimension for masks\n","\n","        # Display images and masks side by side\n","        batch_size = images_np.shape[0]\n","        fig, axes = plt.subplots(batch_size, 2, figsize=(10, 5 * batch_size))\n","\n","        for i in range(batch_size):\n","            axes[i, 0].imshow(images_np[i])\n","            axes[i, 0].set_title(f\"Image {batch_idx * batch_size + i + 1}\")\n","            axes[i, 0].axis(\"off\")\n","\n","            axes[i, 1].imshow(masks_np[i], cmap=\"gray\")\n","            axes[i, 1].set_title(f\"Mask {batch_idx * batch_size + i + 1}\")\n","            axes[i, 1].axis(\"off\")\n","\n","        plt.show()\n","\n","# Use this function to visualize a few batches of images and masks from the train_loader\n","visualize_batch_from_loader(train_loader, num_batches=2)\n"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tuhvlxaWWEXHKQRqpsceVoimmunu_kVS"},"executionInfo":{"elapsed":13431,"status":"ok","timestamp":1732230437532,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"K0eC4o8jF_Xh","outputId":"018f8504-b4ed-4350-a5a2-78f5e8f4c150"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import os\n","from PIL import Image\n","from torchvision import transforms\n","\n","def verify_data_alignment(dataset, image_files, num_samples=5):\n","    \"\"\"\n","    Verify data processing by displaying original images, preprocessed images, and processed masks.\n","\n","    Args:\n","        dataset: The dataset object that provides preprocessed images and masks.\n","        image_files: A list of full paths to the original image files.\n","        num_samples: Number of samples to visualize.\n","    \"\"\"\n","    for i in range(num_samples):\n","        # Retrieve the preprocessed image and mask from the dataset\n","        preprocessed_image, processed_mask = dataset[i]\n","\n","        # Convert tensors to PIL images for visualization\n","        preprocessed_image_pil = transforms.ToPILImage()(preprocessed_image)\n","        processed_mask_pil = transforms.ToPILImage()(processed_mask)\n","\n","        # Load the original image (image_files already contains the full path)\n","        original_image = Image.open(image_files[i]).convert(\"RGB\")\n","\n","        # Display the original image, preprocessed image, and processed mask side by side\n","        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        axs[0].imshow(original_image)\n","        axs[0].set_title(f\"Original Image {i + 1}\")\n","        axs[0].axis(\"off\")\n","\n","        axs[1].imshow(preprocessed_image_pil)\n","        axs[1].set_title(f\"Preprocessed Image {i + 1}\")\n","        axs[1].axis(\"off\")\n","\n","        axs[2].imshow(processed_mask_pil, cmap=\"gray\")\n","        axs[2].set_title(f\"Processed Mask {i + 1}\")\n","        axs[2].axis(\"off\")\n","\n","        plt.show()\n","\n","# Run the verification for the first few images and masks\n","verify_data_alignment(dataset, image_files=image_files, num_samples=10)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"z-mkGqy8F_Xi","executionInfo":{"status":"ok","timestamp":1732219022558,"user_tz":300,"elapsed":3,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        # Apply sigmoid to inputs if not already done\n","        inputs = torch.sigmoid(inputs)\n","\n","        # Flatten\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n","        return 1 - dice\n","\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"dILkM0udF_Xi","executionInfo":{"status":"ok","timestamp":1732219025442,"user_tz":300,"elapsed":1067,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["def calculate_iou_infer(predicted_mask, ground_truth_mask):\n","    \"\"\"\n","    Calculate Intersection over Union (IoU) between the predicted mask and the ground truth mask.\n","\n","    Args:\n","        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n","        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n","\n","    Returns:\n","        float: IoU score.\n","    \"\"\"\n","    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n","    union = np.logical_or(predicted_mask, ground_truth_mask).sum()\n","    return intersection / union if union > 0 else 0.0\n","\n","def calculate_dice_infer(predicted_mask, ground_truth_mask):\n","    \"\"\"\n","    Calculate Dice Coefficient between the predicted mask and the ground truth mask.\n","\n","    Args:\n","        predicted_mask (numpy.ndarray): Binary predicted mask (0 or 1).\n","        ground_truth_mask (numpy.ndarray): Binary ground truth mask (0 or 1).\n","\n","    Returns:\n","        float: Dice Coefficient score.\n","    \"\"\"\n","    intersection = np.logical_and(predicted_mask, ground_truth_mask).sum()\n","    return (2 * intersection) / (predicted_mask.sum() + ground_truth_mask.sum()) if (predicted_mask.sum() + ground_truth_mask.sum()) > 0 else 0.0\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"tmIL6wJfF_Xi","executionInfo":{"status":"ok","timestamp":1732219029379,"user_tz":300,"elapsed":1038,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["import torch.nn as nn\n","# combined dice and BCE loss function\n","# Define Combined Dice and BCE Loss\n","class CombinedDiceBCELoss(nn.Module):\n","    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth=1e-6):\n","        super(CombinedDiceBCELoss, self).__init__()\n","        self.dice_weight = dice_weight\n","        self.bce_weight = bce_weight\n","        self.smooth = smooth\n","        self.bce = nn.BCEWithLogitsLoss()\n","\n","    def forward(self, logits, targets):\n","        # Dice Loss\n","        probs = torch.sigmoid(logits)\n","        intersection = (probs * targets).sum(dim=(1, 2))  # Sum over spatial dimensions only\n","        dice_loss = 1 - (2. * intersection + self.smooth) / (probs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) + self.smooth)\n","        dice_loss = dice_loss.mean()  # Average over the batch\n","\n","        # BCE Loss\n","        bce_loss = self.bce(logits, targets)\n","\n","        # Combine losses\n","        return self.dice_weight * dice_loss + self.bce_weight * bce_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aelEEawiF_Xj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1732230519157,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"4-L1Bkm-F_Xj","outputId":"99167a88-5231-42fa-8236-5f7a5d0c4c32"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import torch.nn.functional as F\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# print the device\n","print(device)\n","\n","# Move the model to the device\n","model.to(device)\n","\n","# Set a smaller learning rate for fine-tuning\n","optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n","\n","# Assuming DiceLoss is already defined in your environment\n","criterion = DiceLoss()\n","\n","def train(model, train_loader, criterion, optimizer, num_epochs=5):\n","    \"\"\"\n","    Training function with loss calculation.\n","    \"\"\"\n","    model.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        for pixel_values, masks in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\"):\n","            # Move inputs and masks to the correct device\n","            pixel_values = pixel_values.to(device, dtype=next(model.parameters()).dtype)  # Match model dtype\n","            masks = masks.to(device, dtype=torch.float32)  # Ensure masks are on the correct device and float32\n","\n","            # Forward pass\n","            outputs = model(pixel_values=pixel_values)\n","            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n","\n","            # Resize logits to match masks\n","            tissue_logits_resized = F.interpolate(\n","                tissue_logits.unsqueeze(1),  # Add channel dimension\n","                size=masks.shape[-2:],  # Match mask size\n","                mode=\"bilinear\",\n","                align_corners=False\n","            ).squeeze(1)  # Remove channel dimension\n","\n","            # Compute loss\n","            loss = criterion(tissue_logits_resized, masks)\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Accumulate loss\n","            running_loss += loss.item()\n","\n","        # Average loss for the epoch\n","        avg_loss = running_loss / len(train_loader)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n","\n","def validate(model, val_loader, criterion, device):\n","    \"\"\"\n","    Validation function aligned with inference logic, including IoU and Dice metric calculation.\n","\n","    Args:\n","        model: The trained segmentation model.\n","        val_loader: DataLoader providing validation images and ground truth masks.\n","        criterion: Loss function for evaluation.\n","        device: Computation device (CPU or CUDA).\n","\n","    Returns:\n","        avg_val_loss: Average validation loss.\n","        avg_iou: Average IoU across the validation set.\n","        avg_dice: Average Dice score across the validation set.\n","    \"\"\"\n","    model.eval()\n","    val_loss = 0.0\n","    total_iou = 0.0\n","    total_dice = 0.0\n","    num_samples = 0\n","\n","    with torch.no_grad():\n","        for images, ground_truth_masks in tqdm(val_loader, desc=\"Validation\"):\n","            # Move ground truth masks to device and convert to float\n","            ground_truth_masks = ground_truth_masks.to(device, dtype=torch.float32)\n","\n","            # Forward pass\n","            outputs = model(pixel_values=images.to(device))\n","            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n","\n","            # Resize logits to match ground truth mask size\n","            tissue_logits_resized = torch.sigmoid(F.interpolate(\n","                tissue_logits.unsqueeze(1),  # Add channel dimension\n","                size=ground_truth_masks.shape[-2:],  # Match mask size\n","                mode=\"bilinear\",\n","                align_corners=False\n","            ).squeeze(1))  # Remove channel dimension\n","\n","            # Compute loss\n","            loss = criterion(tissue_logits_resized, ground_truth_masks)\n","            val_loss += loss.item()\n","\n","            # Convert predicted logits to binary masks\n","            predicted_masks = (tissue_logits_resized > 0.5).cpu().numpy().astype(np.uint8)\n","            ground_truth_masks_np = ground_truth_masks.cpu().numpy().astype(np.uint8)\n","\n","            # Compute metrics\n","            for pred, gt in zip(predicted_masks, ground_truth_masks_np):\n","                total_iou += calculate_iou_infer(pred, gt)\n","                total_dice += calculate_dice_infer(pred, gt)\n","                num_samples += 1\n","\n","    # Calculate average loss and metrics\n","    avg_val_loss = val_loss / len(val_loader)\n","    avg_iou = total_iou / num_samples if num_samples > 0 else 0\n","    avg_dice = total_dice / num_samples if num_samples > 0 else 0\n","\n","    print(f\"\\nValidation Loss: {avg_val_loss:.4f}\")\n","    print(f\"Mean IoU: {avg_iou:.4f}\")\n","    print(f\"Mean Dice: {avg_dice:.4f}\")\n","\n","    return avg_val_loss, avg_iou, avg_dice\n"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1732230481095,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"1pEEqkIuXUtM","outputId":"39dfe4b6-d24c-4f06-ba36-33e9cfeb1767"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available:  True\n"]}],"source":["\n","# Check if CUDA is available\n","print(\"CUDA Available: \", torch.cuda.is_available())"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3015737,"status":"ok","timestamp":1732250550296,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"26XKCN2PW8LT","outputId":"7dd0f977-3da0-4ae7-dc5c-cd39ca8e47eb"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [1/10] Training: 100%|| 868/868 [33:28<00:00,  2.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 0.1722\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [2/10] Training: 100%|| 868/868 [33:20<00:00,  2.30s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [2/10], Loss: 0.1328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [3/10] Training: 100%|| 868/868 [33:22<00:00,  2.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [3/10], Loss: 0.1230\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [4/10] Training: 100%|| 868/868 [33:23<00:00,  2.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [4/10], Loss: 0.1199\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [5/10] Training: 100%|| 868/868 [33:20<00:00,  2.30s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [5/10], Loss: 0.1136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [6/10] Training: 100%|| 868/868 [33:19<00:00,  2.30s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [6/10], Loss: 0.1127\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [7/10] Training: 100%|| 868/868 [33:20<00:00,  2.30s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [7/10], Loss: 0.1086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch [8/10] Training: 100%|| 868/868 [33:20<00:00,  2.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [8/10], Loss: 0.1086\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [9/10] Training: 100%|| 868/868 [33:22<00:00,  2.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/10], Loss: 0.1059\n"]},{"output_type":"stream","name":"stderr","text":["Epoch [10/10] Training: 100%|| 868/868 [33:22<00:00,  2.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/10], Loss: 0.1052\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Train model\n","train(model, train_loader, criterion, optimizer, num_epochs=10)\n","\n"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383787,"status":"ok","timestamp":1732273727338,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"n8NvbBfnF_Xj","outputId":"2efe7b8c-29e5-492e-e1cd-f442fa52cdd1"},"outputs":[{"output_type":"stream","name":"stderr","text":["Validation: 100%|| 217/217 [06:23<00:00,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Loss: 0.6597\n","Mean IoU: 0.6043\n","Mean Dice: 0.6935\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6596855485494235, 0.6043169590157902, 0.6934831365173434)"]},"metadata":{},"execution_count":105}],"source":["# validate the model\n","validate(model, val_loader, criterion, device)"]},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_normF_dice_epoch10_batch16\")\n","image_processor.save_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_normF_dice_epoch10_batch16\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYFfXMlpiHcU","executionInfo":{"status":"ok","timestamp":1732273333630,"user_tz":300,"elapsed":1407,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}},"outputId":"bad78712-8341-4101-a102-cd50c362b516"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/mask2former_tissue_segmentation_normF_dice_epoch10_batch16/preprocessor_config.json']"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1719,"status":"ok","timestamp":1732214278268,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"},"user_tz":300},"id":"CXQLfrEsF_Xj","outputId":"48dd33c0-14a8-4b5c-e610-1ddc3e9f8ea0"},"outputs":[{"data":{"text/plain":["['/content/drive/My Drive/mask2former_tissue_segmentation_dice_epoch10_batch16/preprocessor_config.json']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_dice_epoch10_batch16\")\n","image_processor.save_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_dice_epoch10_batch16\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jQm86piF_Xk"},"outputs":[],"source":["# load the model\n","model_test = Mask2FormerForUniversalSegmentation.from_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_dice_epoch10_batch16\")\n","image_processor_test = Mask2FormerImageProcessor.from_pretrained(\"/content/drive/My Drive/mask2former_tissue_segmentation_dice_epoch10_batch16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmhHoHCPF_Xk","outputId":"839120a8-8805-4a63-9899-9365a725ab4c"},"outputs":[{"data":{"text/plain":["['fine-tuned-mask2former/preprocessor_config.json']"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained('fine-tuned-mask2former')\n","image_processor.save_pretrained('fine-tuned-mask2former')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPBZn8XqF_Xk"},"outputs":[],"source":["# load thefine tuned model\n","model_test = Mask2FormerForUniversalSegmentation.from_pretrained('fine-tuned-mask2former')\n","image_processor_test = Mask2FormerImageProcessor.from_pretrained('fine-tuned-mask2former')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaF6YwbXF_Xk"},"outputs":[],"source":["def calculate_iou_infer(pred, target):\n","    \"\"\"\n","    Calculate Intersection over Union (IoU) between two binary masks.\n","    Works with NumPy arrays.\n","    \"\"\"\n","    pred = (pred > 0.5).astype(np.uint8)\n","    target = (target > 0.5).astype(np.uint8)\n","    intersection = np.logical_and(pred, target).sum()\n","    union = np.logical_or(pred, target).sum()\n","    return intersection / (union + 1e-6)  # Avoid division by zero\n","\n","def calculate_dice_infer(pred, target):\n","    \"\"\"\n","    Calculate Dice coefficient between two binary masks.\n","    Works with NumPy arrays.\n","    \"\"\"\n","    pred = (pred > 0.5).astype(np.uint8)\n","    target = (target > 0.5).astype(np.uint8)\n","    intersection = np.logical_and(pred, target).sum()\n","    return (2 * intersection) / (pred.sum() + target.sum() + 1e-6)  # Avoid division by zero"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"9FIkyE3FF_Xk","executionInfo":{"status":"ok","timestamp":1732230025393,"user_tz":300,"elapsed":426,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image\n","\n","\n","def infer_and_display(model, image_processor, dataloader, device, num_samples=5, target_size=(128, 128)):\n","    \"\"\"\n","    Perform inference and display the original image, ground truth mask, and predicted mask side by side.\n","\n","    Args:\n","        model: The trained segmentation model.\n","        image_processor: Preprocessing module for the input images.\n","        dataloader: DataLoader providing images and ground truth masks.\n","        device: Computation device (CPU or CUDA).\n","        num_samples: Number of samples to visualize.\n","        target_size: Target size for resizing masks during visualization.\n","    \"\"\"\n","    model.eval()\n","    samples_displayed = 0\n","    total_iou = 0.0\n","    total_dice = 0.0\n","    num_evaluated = 0\n","\n","    with torch.no_grad():\n","        for images, ground_truth_masks in tqdm(dataloader, desc=\"Inferencing\"):\n","            # Move ground truth masks to device and convert to float\n","            ground_truth_masks = ground_truth_masks.to(device, dtype=torch.float32)\n","\n","            # Convert tensors to PIL images for the image processor\n","            pil_images = [transforms.ToPILImage()(img) for img in images]\n","            inputs = image_processor(images=pil_images, return_tensors=\"pt\")\n","            pixel_values = inputs[\"pixel_values\"].to(device)\n","\n","            # Forward pass\n","            outputs = model(pixel_values=pixel_values)\n","            tissue_logits = outputs.masks_queries_logits[:, 1]  # Binary segmentation logits\n","\n","            # Resize logits to match mask size\n","            tissue_logits_resized = torch.sigmoid(F.interpolate(\n","                tissue_logits.unsqueeze(1),\n","                size=ground_truth_masks.shape[-2:],  # Match ground truth mask size\n","                mode=\"bilinear\",\n","                align_corners=False\n","            ).squeeze(1))  # Remove channel dimension\n","\n","            # Convert predicted logits to binary masks\n","            predicted_masks = (tissue_logits_resized > 0.5).cpu().numpy().astype(np.uint8)\n","\n","            # Display the first few samples\n","            for i in range(len(images)):\n","                if samples_displayed >= num_samples:\n","                    # Print the average metrics\n","                    avg_iou = total_iou / num_evaluated\n","                    avg_dice = total_dice / num_evaluated\n","                    print(f\"\\nMean IoU: {avg_iou:.4f}\")\n","                    print(f\"\\nMean Dice: {avg_dice:.4f}\")\n","                    return\n","\n","                # Convert images, ground truths, and predictions to displayable formats\n","                original_image = pil_images[i]\n","                ground_truth_mask = ground_truth_masks[i].cpu().numpy().squeeze()  # Remove extra dimensions\n","                predicted_mask = predicted_masks[i]\n","\n","                # Ensure ground truth mask is binary (0 or 1) and scaled to 255\n","                ground_truth_mask = (ground_truth_mask > 0.5).astype(np.uint8) * 255\n","\n","                # Calculate IoU and Dice for the current sample\n","                iou = calculate_iou_infer(predicted_mask, ground_truth_mask // 255)  # Divide by 255 for binary comparison\n","                dice = calculate_dice_infer(predicted_mask, ground_truth_mask // 255)  # Divide by 255 for binary comparison\n","                total_iou += iou\n","                total_dice += dice\n","                num_evaluated += 1\n","\n","                # Resize ground truth and predicted masks for visualization\n","                ground_truth_mask_resized = np.array(Image.fromarray(ground_truth_mask).resize(target_size, Image.NEAREST))\n","                predicted_mask_resized = np.array(Image.fromarray((predicted_mask * 255).astype(np.uint8)).resize(target_size, Image.NEAREST))\n","\n","                # Display side by side\n","                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","                axs[0].imshow(original_image)\n","                axs[0].set_title(f\"Original Image {samples_displayed + 1}\")\n","                axs[0].axis(\"off\")\n","\n","                axs[1].imshow(ground_truth_mask_resized, cmap=\"gray\", vmin=0, vmax=255)\n","                axs[1].set_title(f\"Ground Truth Mask {samples_displayed + 1}\")\n","                axs[1].axis(\"off\")\n","\n","                axs[2].imshow(predicted_mask_resized, cmap=\"gray\", vmin=0, vmax=255)\n","                axs[2].set_title(f\"Predicted Mask {samples_displayed + 1}\")\n","                axs[2].axis(\"off\")\n","\n","                plt.show()\n","                samples_displayed += 1\n","\n","    # Print the final average metrics\n","    avg_iou = total_iou / num_evaluated if num_evaluated > 0 else 0\n","    avg_dice = total_dice / num_evaluated if num_evaluated > 0 else 0\n","    print(f\"\\nMean IoU: {avg_iou:.4f}\")\n","    print(f\"Mean Dice: {avg_dice:.4f}\")"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wqgTm9wqqjBv5lyWjiVLyGqxvAQHJBoR"},"id":"C8UwivGCF_Xl","outputId":"a3e5f27b-5582-4db3-f1b9-bc2a33e79f71","executionInfo":{"status":"ok","timestamp":1732274300434,"user_tz":300,"elapsed":51060,"user":{"displayName":"Brandon Leblanc","userId":"09975613732351395001"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Perform inference and display results\n","\n","model.to(device)\n","\n","infer_and_display(model, image_processor, val_loader, device, num_samples=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_cmk4YbF_Xl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
